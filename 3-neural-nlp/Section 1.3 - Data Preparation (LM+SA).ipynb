{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6012db6-983e-4f82-a45e-e0b4fd2f51f2",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a979c-2ecd-46f9-8758-89cc53812d9b",
   "metadata": {},
   "source": [
    "# Data Preparation: Sentiment Analysis + Language Models\n",
    "\n",
    "Training language models using Neural Network architectures from scratch is a bit more data intensive. We therefore do not include the raw dataset here but provide this notebook for you to generate the training data from the raw data. This also allows you (a) to modify the preprocessing step but also (b) to use a completely different corpus. This would of course require some changes to the code to accommodate for the folder and file structure of the new corpus, but the subsequent steps of generating the training dataset should more or less remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc1d5c6",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b984a075-4aaf-4e49-8df1-27bccc43c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c5b542-4a68-4656-932a-7054010abf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17422af9-6981-47a0-963e-3e47aeed0d58",
   "metadata": {},
   "source": [
    "We utilize some utility methods from PyTorch as well as Torchtext, so we need to import the `torch` and `torchtext` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bf4cda-fccc-43ea-8f75-903ef72a3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f9e1f",
   "metadata": {},
   "source": [
    "As usual, we rely on spaCy to perform basic text preprocessing and cleaning steps, mainly tokenization and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9801c28d-a106-464c-8b6e-83bf821194f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Tell spaCy to use the GPU (if available)\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c2a4e-53ec-4e3e-a48c-389a1a844de8",
   "metadata": {},
   "source": [
    "Lastly, `src/utils.py` provides some utility methods to download and decompress files. Since the datasets used in some of the notebooks are of considerable size -- although far from huge -- they are not part of the repository and need to be downloaded (and optionally decompressed) separately. The 2 methods `download_file` and `decompress_file` accomplish this for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d95dbf2-df76-481a-8fa6-7dfea1a06853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import download_file, decompress_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa8c57-9cdc-48f0-85a7-a76ad505e14f",
   "metadata": {},
   "source": [
    "**Important:** The code cells below to download the file naturally include the URLs of the files. However, there is always the chance that one of those files might be removed or renamed, in which case the URL will now longer be valid. In this case, it is recommended to search for alternative links using, e.g., Google or Bing, which should cause no problems as all datasets used here are generally widely available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2442b60c-ed7a-4298-b1ae-354e016e1d14",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c017c4-8b12-4755-aae2-4aff5a6cfb02",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b32d09",
   "metadata": {},
   "source": [
    "The [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/), commonly known as the IMDb dataset, is a widely used dataset for sentiment analysis and text classification tasks. It was created by Andrew Maas and his team at Stanford University and is freely available for research purposes. The dataset consists of movie reviews collected from the IMDb website, a popular online movie database. The reviews are labeled with sentiment polarity, indicating whether the review expresses a positive or negative sentiment towards the movie. The dataset is often used to train and evaluate machine learning models for sentiment analysis.\n",
    "\n",
    "The dataset contains a total of 50,000 movie reviews, divided into 25,000 reviews for training and 25,000 reviews for testing. Each set is further split into an equal number of positive and negative reviews. The dataset provides a balanced distribution of sentiments. The reviews in the dataset are stored as individual text files, with each file representing a single review. The directory structure organizes the reviews into separate folders for positive and negative sentiments.\n",
    "\n",
    "The Large Movie Review Dataset has been widely used in natural language processing (NLP) research and has served as a benchmark for sentiment analysis tasks. It offers a valuable resource for training and evaluating models in the field of sentiment analysis and text classification.\n",
    "\n",
    "The code cell below should download and decompress the dataset. We recommend using the given `target_path` as this won't require any additional changes in subsequent code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d8d15a-a523-4a38-b927-705fe0dcbb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 84.1M/84.1M [01:49<00:00, 770kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompress file...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Download file...')\n",
    "download_file('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', target_path='data/corpora/imdb-reviews/')\n",
    "print('Decompress file...')\n",
    "decompress_file('data/corpora/imdb-reviews/aclImdb_v1.tar.gz', target_path='data/corpora/imdb-reviews/')\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66acbf96-628a-4106-9930-5220deed4abe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9502fb93",
   "metadata": {},
   "source": [
    "## Dataset Preparation: Language Model\n",
    "\n",
    "**Important Comments:**\n",
    "\n",
    "* Using the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to train a language model has of course its limitations. Firstly, the dataset is very small for this task, and maybe more importantly using a dataset from a specific domain (i.e.: movie reviews) limits its applicability this this domain. However, the focus here is to go through some of the basic steps and not to build a state-of-the-art language model.\n",
    "\n",
    "* The goal in later notebooks is building a simple language model to generate single sentences, not paragraphs or even beyond. Hence each data sample we generate will reflect a single sentence. For training very large language models to generate paragraphs, samples are typically chunks of text containing multiple sentences that might even be arbitrarily cut off.\n",
    "\n",
    "### Auxiliary Method for Data Preprocessing\n",
    "\n",
    "The method `process_file_lm` processes each review file for the use of the document as part of the dataset for learning a language model. Since the movie reviews can include HTML tags, we remove those as well using RegEx. Apart from that, we only lowercase all words. Of course, since we want to use the data for training a language model, we do not perform additional steps such as lemmatization or stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2bc59bb-3fbf-4fd8-9262-2d7143987851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'liked', 'the', 'film', '.'],\n",
       " ['some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'action',\n",
       "  'scenes',\n",
       "  'were',\n",
       "  'very',\n",
       "  'interesting',\n",
       "  ',',\n",
       "  'tense',\n",
       "  'and',\n",
       "  'well',\n",
       "  'done',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'especially',\n",
       "  'liked',\n",
       "  'the',\n",
       "  'opening',\n",
       "  'scene',\n",
       "  'which',\n",
       "  'had',\n",
       "  'a',\n",
       "  'semi',\n",
       "  'truck',\n",
       "  'in',\n",
       "  'it',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'very',\n",
       "  'tense',\n",
       "  'action',\n",
       "  'scene',\n",
       "  'that',\n",
       "  'seemed',\n",
       "  'well',\n",
       "  'done',\n",
       "  '.'],\n",
       " ['some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'transitional',\n",
       "  'scenes',\n",
       "  'were',\n",
       "  'filmed',\n",
       "  'in',\n",
       "  'interesting',\n",
       "  'ways',\n",
       "  'such',\n",
       "  'as',\n",
       "  'time',\n",
       "  'lapse',\n",
       "  'photography',\n",
       "  ',',\n",
       "  'unusual',\n",
       "  'colors',\n",
       "  ',',\n",
       "  'or',\n",
       "  'interesting',\n",
       "  'angles',\n",
       "  '.'],\n",
       " ['also', 'the', 'film', 'is', 'funny', 'is', 'several', 'parts', '.'],\n",
       " ['i',\n",
       "  'also',\n",
       "  'liked',\n",
       "  'how',\n",
       "  'the',\n",
       "  'evil',\n",
       "  'guy',\n",
       "  'was',\n",
       "  'portrayed',\n",
       "  'too',\n",
       "  '.'],\n",
       " ['i', \"'d\", 'give', 'the', 'film', 'an', '8', 'out', 'of', '10', '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_file_lm(file_name):\n",
    "    \n",
    "    text = None\n",
    "    with open(file_name, 'r') as file:\n",
    "        text = file.read().replace('\\n', '')\n",
    "        \n",
    "    # Just a fail-safe if anything is off here\n",
    "    if text is None:\n",
    "        return\n",
    "\n",
    "    ## Remove HTML tags\n",
    "    p = re.compile(r'<.*?>')\n",
    "    text = p.sub(' ', text)\n",
    "    \n",
    "    ## Return \"proper tokens\" (lemme, lowercase)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    samples = []\n",
    "    for sent in doc.sents:\n",
    "        samples.append([ t.text.lower() for t in sent if t.text.strip() != '' ])\n",
    "            \n",
    "    return samples\n",
    "\n",
    "process_file_lm('data/corpora/imdb-reviews/aclImdb/train/pos/1000_8.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55035d",
   "metadata": {},
   "source": [
    "### Preprocessing Review Files\n",
    "\n",
    "The method `process_reviews_lm()` iterates over all text files representing the movie reviews in the specified folders, see above. For each review, we first extract all the tokens using `process_file_lm()`. For each token, we also keep track of its count. We only need this to later create the final vocabulary by only looking at the top-k (e.g., top-20k most frequent) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7cc491-8321-4e4e-a66b-516fdd5ed7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews_lm(folders, num_reviews):\n",
    "    sentences = []             # List of all sentences\n",
    "    token_counter = Counter()  # Dictionary with all tokens and their frequencies\n",
    "    review_count = 0           # Running counter of process reviews\n",
    "    # Iterate over all reviews\n",
    "    with tqdm(total=num_reviews) as progress_bar:\n",
    "        for folder in folders:\n",
    "            for file_name in os.scandir(folder):\n",
    "                # Ignore directories (just a fail-safe; not really needed)\n",
    "                if file_name.is_file() is False:\n",
    "                    continue\n",
    "                # Preprocess review\n",
    "                review_sentences = process_file_lm(file_name.path)\n",
    "                # Add all extracted sentences to final list\n",
    "                sentences.extend(review_sentences)\n",
    "                # Update token counts\n",
    "                for sentence in review_sentences:\n",
    "                    for token in sentence:\n",
    "                        token_counter[token] += 1\n",
    "                # Update progress bar\n",
    "                progress_bar.update(1)\n",
    "                # Check if we need to stop early\n",
    "                review_count += 1\n",
    "                if review_count >= num_reviews:\n",
    "                    return sentences, token_counter\n",
    "    # Return sentences and token counts\n",
    "    return sentences, token_counter                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe035c-1669-48f5-b23f-c6816451a449",
   "metadata": {},
   "source": [
    "For training a language model, we only need the reviews themselves but not the sentiment labels. This means we can make use of the complete dataset, including the 50k reviews that do not have a sentiment label associated with them. Thus, in the code cell below, we include all subfolders for consideration.\n",
    "\n",
    "For testing, we recommend using a lower value for `num_reviews` (e.g., 1000) to see if this and the other notebooks are working (of course, the results won't be great). Once you think all is good, you can set `num_reviews` to infinity to work on the whole dataset.\n",
    "\n",
    "**Side note:** If you used a different target path when decompressing the files, you need to set the `corpus_base_path` variable accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "882b4b66-9d15-4709-8c06-582237d93769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 100000\n"
     ]
    }
   ],
   "source": [
    "corpus_base_path = 'data/corpora/imdb-reviews/'\n",
    "\n",
    "folders = [\n",
    "    corpus_base_path+'aclImdb/test/pos',\n",
    "    corpus_base_path+'aclImdb/test/neg',    \n",
    "    corpus_base_path+'aclImdb/train/pos',\n",
    "    corpus_base_path+'aclImdb/train/neg',\n",
    "    corpus_base_path+'aclImdb/train/unsup'    \n",
    "]\n",
    "\n",
    "num_reviews = 0\n",
    "\n",
    "for folder in folders:\n",
    "    num_reviews += sum([len(files) for r, d, files in os.walk(folder)])\n",
    "\n",
    "num_reviews = min(num_reviews, 999999999)    \n",
    "    \n",
    "print(\"Total number of reviews: {}\".format(num_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc7c74-7734-43b9-9063-82f9dcc517de",
   "metadata": {},
   "source": [
    "Se, let's call the method `process_reviews_lm` to process the reviews. Depending on the number of reviews you have specified and the performance of your machine, this might take several minutes up to 1h+. On the other hand, this should only be a one-time task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc5ee98-9ba9-450e-895d-fb82030984a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [1:18:37<00:00, 21.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 1271285\n",
      "Number of unique tokens: 174318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences, token_counter = process_reviews_lm(folders, num_reviews)\n",
    "\n",
    "print('Total number of sentences: {}'.format(len(sentences)))\n",
    "print('Number of unique tokens: {}'.format(len(token_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca2dfd7",
   "metadata": {},
   "source": [
    "### Create & Save Vocabulary\n",
    "\n",
    "For using the dataset to train a PyTorch model, we need to map each unique word/token to a unique index (i.e., integer identifier). Given a vocabulary size of `V` these unique indices must be of the range from `0` to `V-1`. This is needed since at the end, training a model using the data comes to matrix/tensor operations and we use identifiers to index the respective tensors. Also, we often want to do additional steps such as considering only the top-k most frequent tokens. Again, it's not difficult to implement this from scratch, however, the `torchtext` text simplifies this resulting in cleaner code.\n",
    "\n",
    "The method `process_reviews_lm` already returns the number of occurrences for each token. So if we want to limit the total number of tokens, we simply need to pick the most frequent tokens using those counts. The code cell below accomplishes this, considering the top 20k tokens by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff959cfd-9050-465b-aafc-2838190f58e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_TOKENS = 20000\n",
    "\n",
    "# Sort with respect to frequencies\n",
    "token_counter_sorted = sorted(token_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Consider only the TOP_TOKENS and convert to an OrderedDict (expect by Torchtext; see below)\n",
    "token_ordered_dict = OrderedDict(token_counter_sorted[:TOP_TOKENS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9620b15-faa9-4398-9e0b-fb86661a7234",
   "metadata": {},
   "source": [
    "We can now create a `vocab` object. In its core, it creates the mappings between the tokens and their indices. It also support some additional useful features:\n",
    "\n",
    "* For many tasks, we need to include special tokens in our vocabulary. For example, we often need a special token (e.g., `<PAD>`) to represent an \"empty\" word we can use to pad sequence (see also the other notebooks). Even more common is a special token (e.g., `UNK`) to represent tokens that haven't been seen when building the vocabulary. Not that the exact string for those tokens do not matter. For example, we could have used, say, `[[[padding]]]` and `[[[unseen]]]`. It's only important that those tokens are unique. In the code cell below, we also add `<SOS>` (start of sequence) and `<EOS>` (end of sequence). These are typically required for tasks such as machine translation. While not needed here, it's no harm having them either.\n",
    "\n",
    "* By using `set_default_index()` we can specify the default index to be used if a sentence we want to transform contains a word not seen before. Most intuitively, we will use the index representing the special token `<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d99ef7b-0dd3-4d41-8486-4152c24253a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 20004\n"
     ]
    }
   ],
   "source": [
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "SOS_TOKEN = '<SOS>'\n",
    "EOS_TOKEN = '<EOS>'\n",
    "\n",
    "SPECIALS = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "\n",
    "vocabulary = vocab(token_ordered_dict, specials=SPECIALS)\n",
    "\n",
    "vocabulary.set_default_index(vocabulary[UNK_TOKEN])\n",
    "\n",
    "print('Number of tokens: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31205598-d919-4b41-b443-39d8d2d1727a",
   "metadata": {},
   "source": [
    "**Side note:** Listing `<PAD>` first ensures that its index will be `0`. While it is not required, it is often assumed to be the padding index and commonly the default value for many utility methods of PyTorch. As such, making sure that `<PAD>` gets the index `0` simplifies later code and making it less prone to errors.\n",
    "\n",
    "We also need to save the vocabulary to save the mappings between the tokens and their indices. Without it, we would only have a dataset of integer sequences without knowing which words those integers represent. We can still train a model -- after all, this is why we vectorize the dataset to begin with -- however, then we could not decode predicted sequences of indices back into proper words/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5dca087-6801-4585-9818-bd4a4f180c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file_name = corpus_base_path+'vectorized-rnn-lm/imdb-rnn-lm-{}.vocab'.format(TOP_TOKENS)\n",
    "\n",
    "torch.save(vocabulary, vocabulary_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b0183",
   "metadata": {},
   "source": [
    "### Vectorizing & Saving Dataset\n",
    "\n",
    "In practice, we often deal with very large datasets. This means that creating the vocabulary and vectorizing the corpus can take a significant amount of time -- note this also includes any preprocessing. It is therefore common to consider this as an individual step and save the vectorized dataset to be used for training later. Each sample is a sentence represented by the indices of the contained words, and the length of the sentence. We use this information during training to better handle sequences/sentences of different lengths.\n",
    "\n",
    "As an additional step, we only consider sentences of a certain length (between 5 and 50). This is a somewhat arbitrary choice, and the sole purpose is to limit the dataset to \"normal looking\" sentences.\n",
    "\n",
    "**Side note:** In the code cells below, we use a naming scheme to reflect the number of tokens in the vocabulary (excluding the special tokens). Such naming schemes can be useful when the same raw input data gets converted into different datasets using different preprocessing steps of vocabulary settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2192566-2a1e-4eb4-aa8b-84b1f82057a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len, max_len = 5, 50\n",
    " \n",
    "# Define ouput file\n",
    "output_file = open(corpus_base_path+'vectorized-rnn-lm/imdb-rnn-lm-sentences.txt', 'w')\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Get length of sentence\n",
    "    sent_len = len(sentence)\n",
    "    \n",
    "    # If the sentence is too short or too long, ignore\n",
    "    if sent_len < min_len or sent_len > max_len:\n",
    "        continue\n",
    "\n",
    "    # Convert tokens to their respective indices\n",
    "    sentence_vectorized = vocabulary.lookup_indices(sentence)\n",
    "    \n",
    "    # Save vectorize sentence and length information to the output file\n",
    "    output_file.write('{},{}\\n'.format(' '.join([ str(idx) for idx in sentence_vectorized ]), sent_len))\n",
    "                      \n",
    "output_file.flush()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a9a825",
   "metadata": {},
   "source": [
    "Now we have a ready dataset to train a simple language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc01ab-71c5-4b60-81c4-958041e9ad2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf42e50-2049-40d7-a9f2-c75ff59cca44",
   "metadata": {},
   "source": [
    "## Data Preparation: Sentiment Analysis\n",
    "\n",
    "### Auxiliary Methods for Data Cleaning & Preprocessing\n",
    "\n",
    "As the dataset is organized using different subfolders, which in turn indicate the sentiment label, and each review is represented by its own file containing noise such as HTML tags, we first define a couple of auxiliary methods. Firstly, `get_sentiment_label` takes a complete file name as input and extracts the sentiment label from the path. We have only 2 classes, and here we assign positive reviews the label `1` and negative reviews the label `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "798c5153-0306-4deb-9734-8020ca616a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentiment_label(file_name):\n",
    "    label = file_name.split('/')[-2]\n",
    "    if label.lower() == 'pos':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Let's quickly check the method using an example file name\n",
    "get_sentiment_label('data/corpora/imdb-reviews/aclImdb/train/pos/1000_8.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167224b-688d-4057-b87f-80670686e0c9",
   "metadata": {},
   "source": [
    "Secondly, we again need a method to clean and preprocess a review file. The method `process_file_sa()` below is very similar to the method  `process_file_lm()`; the main difference is in the actual preprocessing steps applied. By default, each token is lemmatized and all punctuation marks are removed. Of course, this does not mean that these selected steps guarantee the best results. So feel free to change them. Note that no stopword removal is performed to preserve words such as *\"not\"*, *\"n't\"*, *\"never\"*, etc. which are intuitively relevant for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3c3a669-077b-4af1-bf9c-3f1c08250af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i',\n",
       "  'be',\n",
       "  'not',\n",
       "  'sure',\n",
       "  'that',\n",
       "  'this',\n",
       "  'comment',\n",
       "  'contain',\n",
       "  'an',\n",
       "  'actual',\n",
       "  'spoiler',\n",
       "  'but',\n",
       "  'i',\n",
       "  'be',\n",
       "  'play',\n",
       "  'it',\n",
       "  'safe',\n",
       "  'so',\n",
       "  'do',\n",
       "  'not',\n",
       "  'read',\n",
       "  'this',\n",
       "  'if',\n",
       "  'you',\n",
       "  'have',\n",
       "  'not',\n",
       "  'see',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'i',\n",
       "  'adore',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'and',\n",
       "  'so',\n",
       "  'do',\n",
       "  'everyone',\n",
       "  'i',\n",
       "  'work',\n",
       "  'with',\n",
       "  'and',\n",
       "  'that',\n",
       "  'be',\n",
       "  'the',\n",
       "  'point',\n",
       "  'i',\n",
       "  'spend',\n",
       "  'a',\n",
       "  'large',\n",
       "  'part',\n",
       "  'of',\n",
       "  'my',\n",
       "  'work',\n",
       "  'life',\n",
       "  'in',\n",
       "  'cinema',\n",
       "  'without',\n",
       "  'be',\n",
       "  'an',\n",
       "  'actor',\n",
       "  'such',\n",
       "  'people',\n",
       "  'be',\n",
       "  'the',\n",
       "  '_',\n",
       "  'sung',\n",
       "  'hero',\n",
       "  'of',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'the',\n",
       "  'gaffer',\n",
       "  'the',\n",
       "  'puller',\n",
       "  'the',\n",
       "  'on',\n",
       "  'air',\n",
       "  'director',\n",
       "  'the',\n",
       "  'lighter',\n",
       "  'and',\n",
       "  'writer',\n",
       "  'the',\n",
       "  'costume',\n",
       "  'people',\n",
       "  'etc',\n",
       "  'etc',\n",
       "  'and',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'thing',\n",
       "  'be',\n",
       "  'tell',\n",
       "  'from',\n",
       "  'their',\n",
       "  'point',\n",
       "  'of',\n",
       "  'view',\n",
       "  'at',\n",
       "  'least',\n",
       "  'to',\n",
       "  'a',\n",
       "  'great',\n",
       "  'extent',\n",
       "  'most',\n",
       "  'actor',\n",
       "  'be',\n",
       "  'nuts',\n",
       "  'and',\n",
       "  'self',\n",
       "  'absorb',\n",
       "  'to',\n",
       "  'the',\n",
       "  'point',\n",
       "  'of',\n",
       "  'absurdity',\n",
       "  'which',\n",
       "  'be',\n",
       "  'what',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'spoofs',\n",
       "  'so',\n",
       "  'well',\n",
       "  'but',\n",
       "  'you',\n",
       "  'have',\n",
       "  'to',\n",
       "  'have',\n",
       "  'work',\n",
       "  'with',\n",
       "  'actor',\n",
       "  'to',\n",
       "  'recognize',\n",
       "  'that',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'be',\n",
       "  'real',\n",
       "  'life',\n",
       "  'drama',\n",
       "  'possible',\n",
       "  'spoiler',\n",
       "  'alert',\n",
       "  'in',\n",
       "  'one',\n",
       "  'great',\n",
       "  'scene',\n",
       "  'the',\n",
       "  'two',\n",
       "  'lead',\n",
       "  'both',\n",
       "  'actor',\n",
       "  'be',\n",
       "  '_',\n",
       "  'discuss',\n",
       "  '_',\n",
       "  'how',\n",
       "  'to',\n",
       "  '_',\n",
       "  'discuss',\n",
       "  'something',\n",
       "  'personal',\n",
       "  'something',\n",
       "  'entirely',\n",
       "  'out',\n",
       "  'of',\n",
       "  'script',\n",
       "  'with',\n",
       "  'another',\n",
       "  'actor',\n",
       "  'and',\n",
       "  'they',\n",
       "  'start',\n",
       "  'make',\n",
       "  'up',\n",
       "  'line',\n",
       "  'rehearse',\n",
       "  'they',\n",
       "  'and',\n",
       "  'critique',\n",
       "  'each',\n",
       "  'other',\n",
       "  \"'s\",\n",
       "  'performance',\n",
       "  'since',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'appear',\n",
       "  'in',\n",
       "  'what',\n",
       "  'be',\n",
       "  'it',\n",
       "  '91',\n",
       "  'it',\n",
       "  'have',\n",
       "  'become',\n",
       "  'fashionable',\n",
       "  'to',\n",
       "  'do',\n",
       "  'this',\n",
       "  'especially',\n",
       "  'on',\n",
       "  'tv',\n",
       "  'but',\n",
       "  'hardly',\n",
       "  'anyone',\n",
       "  'have',\n",
       "  'do',\n",
       "  'it',\n",
       "  'so',\n",
       "  'well'],\n",
       " 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_file_sa(file_name):\n",
    "    \n",
    "    text = None\n",
    "    with open(file_name, 'r') as file:\n",
    "        text = file.read().replace('\\n', '')\n",
    "        \n",
    "    # Just a fail-safe if anything is off here\n",
    "    if text is None:\n",
    "        return\n",
    "\n",
    "    ## Remove HTML tags\n",
    "    p = re.compile(r'<.*?>')\n",
    "    text = p.sub(' ', text)\n",
    "    \n",
    "    ## Return \"proper tokens\" (lemme, lowercase)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    label = get_sentiment_label(file_name)\n",
    "    sample = [ t.lemma_.lower() for t in doc if t.pos_ not in ['PUNCT'] and t.dep_ not in ['punct'] and t.lemma_.strip() != '']\n",
    "\n",
    "    return sample, label\n",
    "\n",
    "process_file_sa('data/corpora/imdb-reviews/aclImdb/train/pos/1136_8.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a15935-9342-4eea-8c78-d9fd25540487",
   "metadata": {},
   "source": [
    "### Preprocessing Review Files\n",
    "\n",
    "Similar to above, below we define a method `process_reviews_sa()` that iterates over all files in a folder and preprocesses them using the method `process_file_sa()`. Again, we keep track of the counts for all tokens to later limit the size of the final vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c3bd124-e331-4e65-9963-5ad01a923720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews_sa(folders, num_reviews):\n",
    "    samples = []               # List of all sentences\n",
    "    labels =  []               # List of all labels\n",
    "    token_counter = Counter()  # Dictionary with all tokens and their frequencies\n",
    "    # Iterate over all reviews\n",
    "    for folder in folders:\n",
    "        print(folder)\n",
    "        with tqdm(total=num_reviews) as progress_bar:\n",
    "            review_count = 0 # Running counter of process reviews\n",
    "            for file_name in os.scandir(folder):\n",
    "                # Ignore directories (just a fail-safe; not really needed)\n",
    "                if file_name.is_file() is False:\n",
    "                    continue\n",
    "                # Preprocess review\n",
    "                sample, label = process_file_sa(file_name.path)\n",
    "                # Add all extracted sentences to final list\n",
    "                samples.append(sample)\n",
    "                labels.append(label)\n",
    "                # Update token counts\n",
    "                for token in sample:\n",
    "                    token_counter[token] += 1\n",
    "                # Update progress bar\n",
    "                progress_bar.update(1)\n",
    "                # Check if we need to stop early\n",
    "                review_count += 1\n",
    "                if review_count >= num_reviews:\n",
    "                    break\n",
    "    # Return sentences, labels, and token counts\n",
    "    return samples, labels, token_counter       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050b03d-4710-48ea-b7a0-b4968e5f86c8",
   "metadata": {},
   "source": [
    "Since sentiment analysis is a classification task, we can utilize only the reviews that actually have a sentiment label. As the original dataset is also already organized into a training and test set, we have to utilize the required subfolder correctly.\n",
    "\n",
    "Again, for testing the code below, you can first manually limit the values of `num_reviews`. However, note that here this limit is for each folder. After all, we need to ensure that any subset of the dataset remains balanced. In other words, we have to avoid that any subset of the dataset contains only, say, positive reviews. The method `process_reviews_sa()` handles this correctly. For example if `num_reviews=1000` both training and test data will contain 2,000 reviews (1,000 positive and 1,000 negative reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83e608ca-63af-4d5b-a842-7f72c089d75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "corpus_base_path = 'data/corpora/imdb-reviews/'\n",
    "\n",
    "folders_train = [\n",
    "    corpus_base_path+'aclImdb/train/pos',\n",
    "    corpus_base_path+'aclImdb/train/neg'\n",
    "]\n",
    "\n",
    "folders_test = [\n",
    "    corpus_base_path+'aclImdb/test/pos',\n",
    "    corpus_base_path+'aclImdb/test/neg'\n",
    "]\n",
    "\n",
    "num_reviews = sum([len(files) for r, d, files in os.walk(folders_test[0])])\n",
    "num_reviews = min(num_reviews, 999999999)        \n",
    "    \n",
    "print(\"Total number of reviews: {}\".format(num_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd59a99-4269-42b1-bdbc-acaca678ee7e",
   "metadata": {},
   "source": [
    "Again, it's time to call the method `process_reviews_sa` to process the reviews, which might take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8118885b-d02b-4f67-b6aa-6446962a8d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/corpora/imdb-reviews/aclImdb/train/pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [09:46<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/corpora/imdb-reviews/aclImdb/train/neg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [09:40<00:00, 21.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/corpora/imdb-reviews/aclImdb/test/pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [09:34<00:00, 21.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/corpora/imdb-reviews/aclImdb/test/neg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [09:42<00:00, 21.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training samples: 25000\n",
      "Total number of test samples: 25000\n",
      "Number of unique tokens: 72888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "samples_train, labels_train, token_counter = process_reviews_sa(folders_train, num_reviews)\n",
    "samples_test , labels_test,  _             = process_reviews_sa(folders_test , num_reviews)\n",
    "\n",
    "print('Total number of training samples: {}'.format(len(samples_train)))\n",
    "print('Total number of test samples: {}'.format(len(samples_test)))\n",
    "print('Number of unique tokens: {}'.format(len(token_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bcb12-3e12-4e5d-9faa-8082352d632c",
   "metadata": {},
   "source": [
    "### Create & Save Vocabulary\n",
    "\n",
    "With the reviews processed, we can again create the vocabulary, which requires the same steps as above using essentially the same code. So for simplicity, we combine all the code for this into a single code cell; see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed56bccd-dc5b-47c4-a073-ce197214478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_TOKENS = 20000\n",
    "\n",
    "# Sort with respect to frequencies\n",
    "token_counter_sorted = sorted(token_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Consider only the TOP_TOKENS and convert to an OrderedDict (expect by Torchtext; see below)\n",
    "\n",
    "token_ordered_dict = OrderedDict(token_counter_sorted[:TOP_TOKENS])\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "SPECIALS = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "\n",
    "vocabulary = vocab(token_ordered_dict, specials=SPECIALS)\n",
    "\n",
    "vocabulary.set_default_index(vocabulary[UNK_TOKEN])\n",
    "\n",
    "vocabulary_file_name = corpus_base_path+'vectorized-rnn-sa/imdb-rnn-sa-{}.vocab'.format(TOP_TOKENS)\n",
    "\n",
    "torch.save(vocabulary, vocabulary_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddd718-6081-497b-a810-7f5e2a6af75d",
   "metadata": {},
   "source": [
    "## Vectorizing & Saving Dataset\n",
    "\n",
    "The last step is again to vectorize the sentences to generate the final dataset. Compared to above, there are 2 differences:\n",
    "\n",
    "* Here, we do not filter sentences based on the lengths\n",
    "\n",
    "* Since we have a training and test set, we also save them into 2 separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "602de444-2717-452f-8d81-60eff283defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(corpus_base_path+'vectorized-rnn-sa/imdb-rnn-sa-reviews-{}-train.txt'.format(TOP_TOKENS), \"w\")\n",
    "\n",
    "for idx, sample in enumerate(samples_train):\n",
    "\n",
    "    label = labels_train[idx]\n",
    "    \n",
    "    # Convert tokens to their respective indices\n",
    "    sample_vectorized = vocabulary.lookup_indices(sample)\n",
    "    \n",
    "    # Write vectorized sample and label to file\n",
    "    output_file.write('{},{}\\n'.format(' '.join([ str(idx) for idx in sample_vectorized ]), label))        \n",
    "        \n",
    "output_file.flush()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd65d5e4-9291-4718-8053-0f066e95fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(corpus_base_path+'vectorized-rnn-sa/imdb-rnn-sa-reviews-{}-test.txt'.format(TOP_TOKENS), \"w\")\n",
    "\n",
    "for idx, sample in enumerate(samples_test):\n",
    "    \n",
    "    label = labels_test[idx]\n",
    "    \n",
    "    # Convert tokens to their respective indices\n",
    "    sample_vectorized = vocabulary.lookup_indices(sample)\n",
    "\n",
    "    # Write vectorized sample and label to file\n",
    "    output_file.write('{},{}\\n'.format(' '.join([ str(idx) for idx in sample_vectorized ]), label))        \n",
    "        \n",
    "output_file.flush()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e58f5e-4fd2-4b76-974f-6cc43ff1bcad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04601387-abf8-4b82-b6ce-94aa6e550afa",
   "metadata": {},
   "source": [
    "## Pretrained Word Embeddings\n",
    "\n",
    "Word embeddings are numerical representations of words in a continuous vector space. Pretrained word embeddings are vector representations of words that are derived from large corpora of text using unsupervised learning techniques. These embeddings capture semantic and syntactic information about words in a dense vector space, where words with similar meanings or contexts are located closer to each other.\n",
    "\n",
    "Once trained, these word embeddings can be reused in various downstream natural language processing (NLP) tasks, such as text classification, named entity recognition, *sentiment analysis*, and machine translation. By utilizing pretrained word embeddings, models can leverage the learned semantic relationships between words and benefit from transfer learning. Pretrained word embeddings have become popular because they offer several advantages. First, they capture rich semantic information that might be challenging to learn from smaller task-specific datasets. Second, they can help overcome the data sparsity problem, especially when dealing with rare words or out-of-vocabulary (OOV) terms. Lastly, pretrained word embeddings enable faster convergence and improved generalization for downstream NLP tasks.\n",
    "\n",
    "Examples of popular pretrained word embeddings include *Word2Vec*, GloVe (Global Vectors for Word Representation), and fastText. These embeddings are typically available in prebuilt formats and can be readily loaded into models to enhance their performance on various NLP tasks. In later notebooks, we will actually train Word2Vec embeddings from scratch.\n",
    "\n",
    "The notebook introducing and implementing an RNN-based model for sentiment analysis includes an optional step to utilize pretrained word embeddings. Such embeddings based on Word2Vec, GloVe, or fastText are available only for download. For example [http://vectors.nlpl.eu/repository/](http://vectors.nlpl.eu/repository/) is an online repository for pretrained word embeddings. In the code cells below, we download and decompress the ZIP file containing the embeddings used for training the sentiment analysis model. Note that these word embeddings have been trained on a lemmatized text corpus. This matches -- and has to match -- the preprocessing steps of the movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0cb740a-9f13-4dcc-92dc-8a7e33a92585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 575M/575M [17:23<00:00, 551kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompress file...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Download file...')\n",
    "download_file('http://vectors.nlpl.eu/repository/20/5.zip', 'data/embeddings/')\n",
    "print('Decompress file...')\n",
    "decompress_file('data/embeddings/5.zip', 'data/embeddings/')\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46b767-f5c8-4d17-80f7-3da376187d6f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a7aa0-dd2b-44f3-ba17-0623a5f4014f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "While we didn't do anyhting exciting here, this notebook has a couple of useful take-away messages:\n",
    "\n",
    "* For large(r) text corpora it is a good practice to consider the preprocessing and preparation of the final dataset (incl. vectorization) as a separate step that requires a lot of consideration, and can be very time and resource-intensive on its own without any training of neural network models. In the follow-up notebooks, we will utilized the dataset generated in this notebook.\n",
    "\n",
    "* Even when using the same corpus, different tasks are likely to require different preprocessing steps. For example, one of the main differences in this notebook was that we lemmatize the data for sentiment analysis (arguably debateable) but not for training language models (arguably mandatory).\n",
    "\n",
    "* The preprocessing and vectorization of text corpora generally involves the same steps. Utilizing and benefitting from well-established packages such as `torchtext` is very recommended. The provided methods are mostly very flexible. Only in the case of very non-standard preprocessing and vectorization steps, any custom implementations should be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e8f9f-3daf-42f5-a2e9-9e7076c2a527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
