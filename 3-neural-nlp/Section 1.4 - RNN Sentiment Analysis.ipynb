{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0350e364-fc9d-45c4-82cd-3acac09894d9",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f415b-0a2d-4fc5-81a6-5ffd853a032a",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs): Sentiment Analysis\n",
    "\n",
    "Sentiment analysis, also known as opinion mining, is an NLP task that involves determining the sentiment, emotion, or subjective tone expressed in a piece of text. It aims to automatically analyze and classify the sentiment conveyed by the text as, for example *positive*, *negative*, *neutral*, or sometimes more fine-grained sentiments. The goal of sentiment analysis is to extract and understand the subjective information present in text data, enabling automated systems to comprehend people's opinions, attitudes, and emotions towards various topics, products, services, or events.\n",
    "\n",
    "Sentiment analysis can be considered a text classification task because its goal is to classify or categorize a piece of text into predefined sentiment classes or labels. In sentiment analysis, the text can be a sentence, a document, a review, a tweet, or any other form of textual input. Text classification refers to the process of assigning predefined categories or labels to a given text based on its content. In the case of sentiment analysis, the predefined categories are sentiment classes (e.g. *positive*, *negative*, *neutral*)\n",
    "\n",
    "By treating sentiment analysis as a text classification problem, various classification algorithms and techniques can be leveraged to build models that can effectively analyze and categorize the sentiment in textual data. Text classification techniques, including machine learning algorithms like Naive Bayes, Support Vector Machines (SVM), Random Forests, and deep learning models such as Convolutional Neural Networks (CNNs) or **Recurrent Neural Networks (RNNs)**, can be employed for sentiment analysis. These models take the text as input, extract relevant features, and make predictions about the sentiment label.\n",
    "\n",
    "In this notebook, we train a sentiment classifier based on an RNN model using movie reviews as the training data. As usual, the goal is not to achieve state-of-the-art results but to systematically go through the main stops to solve a classification task using RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821ac59-d8ef-4934-a49b-f3a08068dc08",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79138794-1ed9-4315-a122-77bca80859d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71fb95-a99c-42fb-a98e-ebb72a64c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c1117-3deb-47bb-ad9f-90b97f36ae3b",
   "metadata": {},
   "source": [
    "We utilize PyTorch as our deep learning framework of choice by importing the `torch` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644e0f3-c539-4bdf-a480-fb117085405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace964b-8d1d-4ccb-982b-8e34fa71ccfa",
   "metadata": {},
   "source": [
    "We also need to import some custom implementations of classes and methods. This makes a re-use of these classes and methods easier and keeps the notebook clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c876fe5-2c26-4ebd-92c8-cd86c942f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom BatchSampler\n",
    "from src.sampler import BaseDataset, EqualLengthsBatchSampler\n",
    "# Core implementation of RNN classifier and Attention\n",
    "from src.rnn import RnnTextClassifier, DotAttention\n",
    "# Some utility classes and methods\n",
    "from src.utils import Dict2Class, plot_training_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbcc76-19d3-464e-97ad-a972e4ea10e6",
   "metadata": {},
   "source": [
    "### Checking/Setting the Computation Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPUs to significantly speed up the training process. If you have a support GPU, feel free to utilize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f4cd4-66f7-48b8-b989-87399386266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU (in case you don't have a supported GPU)\n",
    "# With this small dataset and simple model you won't see a difference anyway\n",
    "#use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046669b9-6203-420b-9635-5b5a9b0b6248",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1021e-dbb7-4d8e-94db-5c37d0e0ea72",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "While RNNs allow for arbitrary lengths -- as long as the sequence in the same batch is of the same length -- it is often practical to limit the maximum length of sequences. This is not only from a computing point of view but also it gets more and more difficult to propagate meaningful gradients back during Backpropagation Through Time (BPTT).\n",
    "\n",
    "For a sentence dataset, this is hardly an issue, since individual sentences are usually not overly long. However, the movie reviews  generally consist of several sentences. Note that by limiting ourselves to the first `MAX_LENGTH` words we assume that the main sentiment is expressed at the beginning of the review. If we assume that we should focus on the end of a review, we should consider the last `MAX_LENGTH` words.\n",
    "\n",
    "In the code cell below, we set `MAX_LENGTH` to 100, but feel free to play with this value. When loading the data from the files, we directly cut all sequences longer than `MAX_LENGTH` down to the specified values. This also means that we won't have to check the sequence lengths anymore when training or evaluating a model (compared to CNN). In practice, `MAX_LENGTH` would be an interesting hyperparameter to optimize, but here out of scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38facef-5211-4345-8c02-3fd7aecc351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12bcb1-80bd-4318-bf4f-24fbcb2a3e8d",
   "metadata": {},
   "source": [
    "### Load Data from File\n",
    "\n",
    "We already preprocessed and vectorized the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) in the previous notebook. We essentially only need to load the generated files. Let's start with the vocabulary. Recall, the `vocabulary` is a `vocab` object from the `torchtext` package, allowing us to map words/tokens to their unique integer identifiers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670f072-11d4-4d66-90f0-4553fcc4f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = torch.load('data/corpora/imdb-reviews/vectorized-rnn-sa/imdb-rnn-sa-20000.vocab')\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print('Size of vocabulary:\\t{}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d3d5f-961e-4eae-8704-685fefb520ff",
   "metadata": {},
   "source": [
    "Now we can load the vectorized reviews, which are split across 2 files: one for the training, the other for the test set. The only additional steps we perform below is to cut each review to a length of `MAX_LENGTH`, if needed, and to shuffle both the training and test set. This is in general a good practice, and here strongly recommended since we know that both files first list all positive and then all negative reviews. This would later result in most training batches containing only positive or negative reviews, which typically results in a poor training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b759b3b-ce00-4750-9c04-fa9dd076d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train, samples_test = [], []\n",
    "\n",
    "with open('data/corpora/imdb-reviews/vectorized-rnn-sa/imdb-rnn-sa-reviews-20000-train.txt') as file:\n",
    "    for line in file:\n",
    "        name, label = line.split(',')\n",
    "        # Convert name to a sequence of integers\n",
    "        sequence = [ int(index) for index in name.split() ]\n",
    "        # Add (sequence,label) pair to list of samples\n",
    "        samples_train.append((sequence[:MAX_LENGTH], int(label.strip())))\n",
    "        \n",
    "with open('data/corpora/imdb-reviews/vectorized-rnn-sa/imdb-rnn-sa-reviews-20000-test.txt') as file:    \n",
    "    for line in file:\n",
    "        name, label = line.split(',')\n",
    "        # Convert name to a sequence of integers\n",
    "        sequence = [ int(index) for index in name.split() ]\n",
    "        # Add (sequence,label) pair to list of samples\n",
    "        samples_test.append((sequence[:MAX_LENGTH], int(label.strip())))\n",
    "        \n",
    "random.shuffle(samples_train)\n",
    "random.shuffle(samples_test)\n",
    "        \n",
    "print('Number of training samples: {}'.format(len(samples_train)))\n",
    "print('Number of test samples: {}'.format(len(samples_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc3bca2-d6fb-4d1d-8294-e23cae26ef4c",
   "metadata": {},
   "source": [
    "### Create Training & Test Set\n",
    "\n",
    "Since the dataset comes in 2 files reflecting the training and test data, we can directly convert the dataset into the respective lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8096266-2736-4acc-a7c3-f62bec48f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [ torch.LongTensor(seq) for (seq, _) in samples_train ]\n",
    "X_test  = [ torch.LongTensor(seq) for (seq, _) in samples_test ]\n",
    "\n",
    "y_train = [ label for (_, label) in samples_train ]\n",
    "y_test  = [ label for (_, label) in samples_test ]\n",
    "\n",
    "# We can directly convert the vector of labels to a tensor\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test  = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24591fc6-8d76-433d-b933-adb824a0c1a4",
   "metadata": {},
   "source": [
    "Note that `X_train` and `X_test` are themselves not tensors but a list of tensors, as this would require that all sequences have the same length. While we ensured that no sequence is longer than 100 words/tokens, there still can be reviews shorter than that. As such, `X_train` and `X_test` are not yet ready to feed into a neural network. This we will address next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164f976-282d-4a14-881c-2df422be887e",
   "metadata": {},
   "source": [
    "### Create Data Loaders\n",
    "\n",
    "We first create a simple class called `BaseDataset` extending [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). This class only stores out `inputs` and `targets` and needs to implement the `__len__()` and `__getitem__()` methods. Since our class extends the abstract class [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset), we can use an instance later to create a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). Without going into too much detail, this approach does not only allow for cleaner code but also supports parallel processing on many CPUs, or on the GPU as well as to optimize data transfer between the CPU and GPU, which is critical when processing very large amounts of data. It is therefore the recommended best practice.\n",
    "\n",
    "The [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class takes a `DataSet` object as input to handle to split the dataset into batches. The class `EqualLengthsBatchSampler` analyzes the input sequences to organize all sequences into groups of sequences of the same length. Then, each batch is sampled for a single group, ensuring that all sequences in the batch have the same length. In the following, we use a batch size of 256, although you can easily go higher since we are dealing with only sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dffa6e-2aa4-4516-b037-70a583f4b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "dataset_train = BaseDataset(X_train, y_train)\n",
    "sampler_train = EqualLengthsBatchSampler(batch_size, X_train, y_train)\n",
    "loader_train = DataLoader(dataset_train, batch_sampler=sampler_train, shuffle=False, drop_last=False)\n",
    "\n",
    "dataset_test = BaseDataset(X_test, y_test)\n",
    "sampler_test = EqualLengthsBatchSampler(batch_size, X_test, y_test)\n",
    "loader_test = DataLoader(dataset_test, batch_sampler=sampler_test, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf13eac-9a40-4d8c-bd6d-01e71b829c92",
   "metadata": {},
   "source": [
    "We can now iterate over all batches using a simple loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14118c8b-3d7c-4af5-9e75-a59d7f0613d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in loader_train:\n",
    "    print('X.shape:', X.shape)\n",
    "    print('y.shape:', y.shape)\n",
    "    break # We don't need to see all batches here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f72e71-8e73-4c72-a72c-967b61b0e29b",
   "metadata": {},
   "source": [
    "The shape of `X` reflects the number of samples (i.e., reviews) in the batch, and the length of all sequences in the batch. Note that the number of samples is most of the time much smaller than our specified batch size of 256 (see above). This is because we enforce that all sequences need to have the same length, and if there are less than 256 sequences of the same length in our dataset, the corresponding batch won't be full.\n",
    "\n",
    "**Side note:** There are standard techniques to have batches with sequences of initially different lengths (keyword: *padding*). Here, however, for convenience, we use the approach of packing sequences of the same length in the same batches. Also, it is easy to see that the chance of an underfull batch reduces for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1b53d7-60a8-4ed3-ae0b-479b3c1bf13d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36db0b-5763-4513-b423-4b3ab9ff2bd3",
   "metadata": {},
   "source": [
    "## Training & Evaluating the RNN Model\n",
    "\n",
    "With the training and test set prepared, we are now ready to build our RNN-based sentiment classification model.\n",
    "\n",
    "### Auxiliary Methods\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "The code cell below implements the method `evaluate()` to, well, evaluate our model. Apart from the model itself, the method also receives the data loader as input parameter. This allows us later to use both `loader_train` and `loader_test` to evaluate the training and test loss using the same method.\n",
    "\n",
    "The method is very generic and is not specific to the dataset. It simply loops over all batches of the data loader, computes the log probabilities, uses these log probabilities to derive the predicted class labels, and compares the predictions with the ground truth to return the f1 score. This means, this method could be used \"as is\" or easily be adopted for all kinds of classifications tasks (incl. task with more than 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d8973-b913-419b-8ed6-e73b04b2f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with tqdm(total=len(loader)) as progress_bar:\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            batch_size, seq_len = X_batch.shape[0], X_batch.shape[1]\n",
    "            \n",
    "            # Move the batch to the correct device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Initialize the first hidden state h0 (and move to device)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            if type(hidden) is tuple:\n",
    "                hidden = (hidden[0].to(device), hidden[1].to(device))  # LSTM\n",
    "            else:\n",
    "                hidden = hidden.to(device)  # RNN, GRU\n",
    "                    \n",
    "            # Use model to compute log prbabilities for each class\n",
    "            log_probs = model(X_batch, hidden)\n",
    "\n",
    "            # Pick class with the highest log probability\n",
    "            y_batch_pred = torch.argmax(log_probs, dim=1)\n",
    "\n",
    "            y_true += list(y_batch.cpu().numpy())\n",
    "            y_pred += list(y_batch_pred.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(batch_size)\n",
    "\n",
    "    # Return final f1 score\n",
    "    return f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f7af5-8254-4a4c-a970-c0325b33803c",
   "metadata": {},
   "source": [
    "#### Training (single epoch)\n",
    "\n",
    "Similar to the method `evaluate()` we also implement a method `train_epoch()` to wrap all the required steps training. This has the advantage that we can simply call `train_epochs()` multiple times to proceed with the training. Apart from the model, this method has the following input parameters:\n",
    "\n",
    "* `optimizer`: the optimizer specifier how the computed gradients are used to updates the weights; in the lecture, we only covered the basic Stochastic Gradient Descent, but there are much more efficient alternatives available\n",
    "\n",
    "* `criterion`: this is the loss function; \"criterion\" is just very common terminology in the PyTorch documentation and tutorials\n",
    "\n",
    "The heart of the method is the snippet described as PyTorch Magic. It consists of the following 3 lines of code\n",
    "\n",
    "* `optimizer.zero_grad()`: After each training step for a batch if have to set the gradients back to zero for the next batch\n",
    "\n",
    "* `loss.backward()`: Calculating all gradients using backpropagation\n",
    "\n",
    "* `optimizer.step()`: Update all weights using the gradients and the method of the specific optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396f9ae-5af6-467a-9223-ffa7b44563f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    \n",
    "    # Initialize epoch loss (cummulative loss fo all batchs)\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    with tqdm(total=len(loader)) as progress_bar:\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            batch_size, seq_len = X_batch.shape[0], X_batch.shape[1]\n",
    "\n",
    "            # Move the batch to the correct device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Initialize the first hidden state h0 (and move to device)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            if type(hidden) is tuple:\n",
    "                hidden = (hidden[0].to(device), hidden[1].to(device))  # LSTM\n",
    "            else:\n",
    "                hidden = hidden.to(device)  # RNN, GRU            \n",
    "            \n",
    "            log_probs = model(X_batch, hidden)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(log_probs, y_batch)\n",
    "            \n",
    "            ### Pytorch magic! ###\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Keep track of overall epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            progress_bar.update(batch_size)\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1b176-2acb-4753-bf18-720d3caff0ae",
   "metadata": {},
   "source": [
    "#### Training (multiple epochs)\n",
    "\n",
    "The `train()` method combines the training and evaluation of a model epoch by epoch. The method keeps track of the loss, the training score, and the tests score for each epoch. This allows as later to plot the results; see below. Notice the calls of `model.train()` and `model.eval()` to set the models into the correct \"mode\". This is needed since our model contains a Dropout layer. For more details, check out this [Stackoverflow post](https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86bae9-9ef8-4cf8-b3ad-18566f5348ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_train, loader_test, optimizer, criterion, num_epochs, verbose=False):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Total Training Time (total number of epochs: {})\".format(num_epochs))\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = train_epoch(model, loader_train, optimizer, criterion)\n",
    "        model.eval()\n",
    "        f1_train = evaluate(model, loader_train)\n",
    "        f1_test = evaluate(model, loader_test)\n",
    "\n",
    "        results.append((epoch_loss, f1_train, f1_test))\n",
    "        \n",
    "        if verbose is True:\n",
    "            print(\"[Epoch {}] loss:\\t{:.3f}, f1 train: {:.3f}, f1 test: {:.3f} \".format(epoch, epoch_loss, f1_train, f1_test))\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34458c13-748e-4e7a-85e8-af656c17d372",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "\n",
    "#### Create Model Instance\n",
    "\n",
    "The class `RnnTextClassifier` implements an RNN-based classifier in a flexible manner, using different parameters settings one can set:\n",
    "\n",
    "* Which recurrent cell to use: `nn.RNN`, `nn.GRU`, or `nn.LSTM`\n",
    "\n",
    "* The number of stacked recurrent layers\n",
    "\n",
    "* Whether the recurrence is performed bi-directional or not\n",
    "\n",
    "* The number and size of the subsequence linear layers\n",
    "\n",
    "* ... and other various parameters.\n",
    "\n",
    "You can and should check out the implementation of the class `RnnTextClassifier` in the file `src/rnn.py`. While the code might look a bit complex at a first glance, most of the complexity is purely because of the flexibility the class provides. For example the hidden state for `nn.LSTM` is different compared to `nn.GRU` and `nn.LSTM` and the code has to accommodate for both cases. If we would fix the overall architecture of the model, the class `RnnTextClassifier` would probably comprise only half the amount of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f250c-a730-4d1c-9b0b-8b363148e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"vocab_size\": vocab_size,                  # the size of the vocabulary determines the input size of the embedding\n",
    "    \"embed_size\": 300,                         # needs to be 300 if we want to use the pretrained word embeddings\n",
    "    \"rnn_cell\": \"GRU\",                         # in practice GRU or LSTM will always outperform RNN\n",
    "    \"rnn_num_layers\": 2,                       # 1 or 2 layers are most common; more rarely sees any benefit\n",
    "    \"rnn_bidirectional\": True,                 # if TRUE, we go over each sequence from both directions\n",
    "    \"rnn_hidden_size\": 512,                    # size of the hidden state\n",
    "    \"rnn_dropout\": 0.5,                        # only relevant if rnn_num_layers > 1\n",
    "    \"dot_attention\": False,                    # if TRUE, use attention\n",
    "    \"linear_hidden_sizes\": [128, 64],          # list of sizes of subsequent hidden layers; can be [] (empty)!\n",
    "    \"linear_dropout\": 0.5,                     # if hidden linear layers are used, we can also include Dropout\n",
    "    \"output_size\": 2                           # we only have to sentiment classes\n",
    "}\n",
    "\n",
    "# Define model paramaters\n",
    "params = Dict2Class(params)\n",
    "# Create model   \n",
    "rnn = RnnTextClassifier(params).to(device)\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0001)\n",
    "# Define loss function\n",
    "criterion = nn.NLLLoss()\n",
    "# Print the model\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08000a11-415b-41c9-8731-ccc26234cc76",
   "metadata": {},
   "source": [
    "#### Set Pretrained Word Embeddings (optional)\n",
    "\n",
    "If we want to use pre-trained word embeddings, e.g., Word2Vec, this is the moment to do. A source for pre-trained word embeddings is [this site](http://vectors.nlpl.eu/repository/). When downloading the a file containing pre-trained word embeddings, there are some things to consider:\n",
    "\n",
    "* Most obviously, the pre-trained embeddings should match the language (here: English).\n",
    "\n",
    "* The pretrained embeddings should match the preprocessing steps. For example, we lemmatized our dataset for this notebook (at least by default, maybe you have changed it). So we need embeddings trained over a lemmatized dataset as well.\n",
    "\n",
    "* The pretrained embeddings have to match the size of our embedding layer. So if we create a embedding layer of size 300, we have to use pretrained embeddings of the same size\n",
    "\n",
    "* The files with the pretrained embeddings are too large to ship with the notebooks, so you have to download them separately :)\n",
    "\n",
    "First, we need to load the pretrained embeddings from the file; here I used [this file](http://vectors.nlpl.eu/repository/20/5.zip) (lemmatized, size: 300):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dc4b3-33c4-4abb-9d6c-113c55a935b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vectors = torchtext.vocab.Vectors(\"data/embeddings/model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0f113-256a-4605-be5c-753b8f36207e",
   "metadata": {},
   "source": [
    "Now we have over 270k pretrained word embeddings, but we only have 20k words in our vocabulary. So we need to create an embedding -- which is basically just a $20k \\times 300$ matrix containing the respective 20k pretrained word embeddings for our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df351e-534b-4d73-a426-0716297a7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = pretrained_vectors.get_vecs_by_tokens(vocabulary.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1db7f1-3036-4175-b3a8-33b8ea49e007",
   "metadata": {},
   "source": [
    "Now we can set the weights of the embedding layer of our model to the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0dbc8-fcdb-4c8b-afe4-08f61f9fc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.embedding.weight.data = pretrained_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f6b98-4ea4-4087-bc3f-a1f249fd7127",
   "metadata": {},
   "source": [
    "Lastly, we can decide if we want the pretrained embeddings to remain fixed or whether we want to update them during training. By setting `.requires_grad = False`, we tell the optimizer to \"freeze\" the layer **not** to update the embedding weights during training. You should observe that if we freeze the embedding layer, the training and test f1 score will remain quite similar; otherwise the training f1 score will go towards 1.0, indicating overfitting. Simply try both settings and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88816523-ed7d-434e-9ba0-9f32049b053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn.embedding.weight.requires_grad = False\n",
    "rnn.embedding.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f92ca6-1065-42b6-a134-5fed4d7412c8",
   "metadata": {},
   "source": [
    "Since the embedding weights still reside on the CPU, we can move the model to the respective device so that the model on all data is indeed on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf909a0d-0d07-4c55-89b2-94620d712968",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba5cff-2a07-43f3-9ab2-aa8716e33066",
   "metadata": {},
   "source": [
    "#### Evaluate Untrained Model\n",
    "\n",
    "Let's first see how our model performs when untrained, i.e., with the initial random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd0672-7be0-41cb-992a-5535770d1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = evaluate(rnn, loader_test)\n",
    "\n",
    "print('F1 score for untrained model: {:.3f}'.format(f1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3aa6e2-07ad-4b35-be83-5739487cbfd7",
   "metadata": {},
   "source": [
    "Since our dataset is perfectly balanced w.r.t. to the 2 class labels (50% positive, 50% negative), and assuming that a random model represents a random guesser, we would expect an f1 score of around 0.5. Of course, depending on the random initialization, our model might perform a bit better or worse than a random guess. In principle, even an f1 score of 0.0 is possible -- for example, this can happen if the weights are initialized in such a way that all predictions are of the same class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa6997-5956-4f70-9b19-efa9cc03842a",
   "metadata": {},
   "source": [
    "### Full Training (and evaluation after each epoch)\n",
    "\n",
    "Using the auxiliary methods and all components (i.e., loss function, optimizer) defined above, we can finally train our model by calling the method `train()`. Note that you can run the code cell below multiple times to continue the training for further 10 epochs. Each epoch will print 3 progress bars:\n",
    "\n",
    "* training over training set\n",
    "\n",
    "* evaluating over training set\n",
    "\n",
    "* evaluating over test set\n",
    "\n",
    "After each epoch, a print statement will show the current loss as well as the latest f1 scores for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740d46c-e28a-4952-ba03-ca57af7070ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "#train(basic_rnn_classifier, loader, num_epochs, verbose=True)\n",
    "results = train(rnn, loader_train, loader_test, optimizer, criterion, num_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67766399-fd0e-456d-a9f4-d8e72266cd87",
   "metadata": {},
   "source": [
    "### Plotting the Results\n",
    "\n",
    "Since the method `train()` returns the losses and f1 scores for each epoch, we can use this data to visualize how the loss and the f1 scores change over time, i.e., after each epoch. In `src.utils` you can find the method `plot_training_results()` to plot the losses and accuracies (training + test) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb78a5-6af4-4339-9329-6effc12c3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94df4eb-c73f-4bb7-b1b2-f0c9ee358648",
   "metadata": {},
   "source": [
    "The result and the plot will heavily depend on the exact parameter setting and whether the pretrained word embeddings gets updated or not. In general, however, you should always see the loss going down and (at least) the training f1 score going up. Usually the test f1 score will also go up, at least in the beginning. Of course, if you increase the number of epochs, you are likely to see signs of overfitting with the test f1 score starting to go down again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d951d0e-cdb4-4f45-80eb-530276b06a03",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded1f33-5d00-4a79-a84d-2dbfd75c06f2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Recurrent Neural Networks (RNNs) can be effectively used to train models for sentiment analysis, which involves determining the sentiment or emotion expressed in a piece of text. RNNs are particularly suitable for this task because they can capture sequential information and dependencies within the text. Here's a general approach to training an RNN model for sentiment analysis:\n",
    "\n",
    "* **Data Preparation:** Prepare your dataset by splitting it into training and testing sets. Each input sample should consist of a text sequence (e.g., a sentence or a paragraph) and its corresponding sentiment label (positive or negative).\n",
    "\n",
    "* **Text Preprocessing:** Perform necessary preprocessing steps on the text data, such as tokenization (breaking text into individual words), removing stopwords, and converting words to lowercase. We performed this and the previous step in a separate notebook.\n",
    "\n",
    "* **Word Embeddings:** Transform the preprocessed words into word embeddings. You can use pretrained word embeddings like Word2Vec or GloVe, or train your own embeddings specific to your dataset.\n",
    "\n",
    "* **Sequence Padding:** Pad or truncate the text sequences to a fixed length, ensuring that all input sequences have the same length. This is necessary to create uniform input for the RNN model. In this notebook, we used a custom approach to organize our batches in such a way that all sequences in the batch are guaranteed to have the same lengths.\n",
    "\n",
    "* **Model Architecture:** Define the architecture of your RNN model. A common choice is the Long Short-Term Memory (LSTM) network, which is a variant of RNN that can effectively capture long-term dependencies.\n",
    "\n",
    "* **Model Building:** Construct your RNN model using frameworks like TensorFlow or PyTorch. The model typically consists of an embedding layer to convert word indices to word vectors, followed by one or more LSTM layers, and finally, a dense layer for sentiment classification.\n",
    "\n",
    "* **Training:** Train the RNN model on the training data using techniques like backpropagation and gradient descent. The model learns to capture the patterns and relationships between words in the text sequences and their corresponding sentiments.\n",
    "\n",
    "* **Evaluation:** Evaluate the trained model on the testing data to assess its performance. Common evaluation metrics for sentiment analysis include accuracy, precision, recall, and F1 score.\n",
    "\n",
    "* **Inference:* Once the model is trained and evaluated, you can use it to predict the sentiment of new, unseen text data. The model takes a text sequence as input, processes it through the learned network, and outputs the predicted sentiment label.\n",
    "\n",
    "It's worth noting that the above steps provide a high-level overview, and there can be variations and additional considerations depending on the specific requirements of your sentiment analysis task. However, this general approach using RNNs serves as a foundation for building sentiment analysis models that can effectively analyze and classify sentiment in textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2ea80-ef00-4f00-bb3f-d6b3a13dd09f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
