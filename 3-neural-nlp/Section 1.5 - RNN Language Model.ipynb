{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2489b28-d653-4d46-838f-746b119839d6",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cd846-b939-4163-a36b-d569ee7aecfe",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs): Language Models\n",
    "\n",
    "Language models are computational models that learn the statistical patterns and structures of natural language. These models are trained on large amounts of text data and aim to predict the likelihood of a given word or sequence of words occurring in a given context. Language models can be used for a wide range of NLP tasks, including text generation, machine translation, speech recognition, sentiment analysis, and question answering.\n",
    "\n",
    "The primary purpose of language models is to capture the inherent structure and semantic relationships within language. By learning from vast amounts of textual data, language models develop an understanding of word co-occurrences, syntactic rules, and contextual cues. This knowledge enables them to generate coherent and contextually relevant text, complete sentences, correct grammatical errors, and even provide auto-suggestions while typing.\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are particularly suitable for training language models due to their inherent ability to handle sequential data. Here are some reasons why RNNs are well-suited for language modeling:\n",
    "\n",
    "* **Sequential Processing:** Language is inherently sequential, and the meaning of a word depends on the words that came before it. RNNs process sequences of inputs one element at a time, allowing them to capture the temporal dependencies and context within the text. This sequential processing makes RNNs suitable for tasks like language modeling, where understanding the context is crucial for generating coherent and meaningful text.\n",
    "\n",
    "* **Variable-Length Input:** RNNs can handle inputs of variable lengths, which is important in language modeling since sentences or documents can vary in length. RNNs can process each word or character in a sequential manner and update their hidden state accordingly, accommodating different lengths of input sequences.\n",
    "\n",
    "* **Long-Term Dependencies:** RNNs are capable of capturing long-term dependencies in the input sequence. The hidden state of an RNN at any given time step summarizes the information from all previous time steps. This allows the RNN to retain memory of past inputs and use it to influence the predictions at future time steps. Language models benefit from this property, as they need to consider not just recent context but also distant dependencies in the text.\n",
    "\n",
    "* **Parameter Sharing:** RNNs share the same set of parameters across all time steps, which enables them to generalize well to different parts of the input sequence. This parameter sharing is particularly useful in language modeling, where the same set of weights can be applied to process different words or characters in the text. It allows the model to learn patterns and dependencies in the language more effectively, even when the training data is sparse or the vocabulary is large.\n",
    "\n",
    "* **Efficient Training with Backpropagation Through Time (BPTT):** RNNs can be trained efficiently using the backpropagation through time algorithm. BPTT applies the chain rule of calculus to compute the gradients of the loss function with respect to the model parameters, effectively unrolling the recurrent computations. This enables efficient training of RNNs on long sequences by propagating the gradients through time, updating the parameters based on the accumulated information from the entire sequence.\n",
    "\n",
    "While RNNs have been widely used for language modeling, they do have limitations, such as difficulties in capturing very long-term dependencies and vanishing/exploding gradient problems. To address some of these limitations, variants of RNNs, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have been introduced, which incorporate mechanisms to better capture and control the flow of information through the recurrent connections.\n",
    "\n",
    "In this notebook, we built a simple RNN-based language model. The corpus we use, we generated using the Data Preparation notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52572ab",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4c971f-eaa4-431f-a6bd-93488ec2b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425534a9-10ab-404c-aefd-b4108715f0f4",
   "metadata": {},
   "source": [
    "We utilize PyTorch as our deep learning framework of choice by importing the `torch` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efea976e-4633-4261-a01b-ac9f234b28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9af34-23e5-4da2-a5b0-df6b93f6dc16",
   "metadata": {},
   "source": [
    "We also need to import some custom implementations of classes and methods. This makes a re-use of these classes and methods easier and keeps the notebook clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2097d941-2a92-428a-b18b-0a7781428999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import Dict2Class\n",
    "from src.rnn import VanillaRnnLanguageModel, RnnLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000dbe43",
   "metadata": {},
   "source": [
    "### Checking/Setting the Computation Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPUs to significantly speed up the training process. If you have a support GPU, feel free to utilize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca25e0f7-39d2-4696-9f57-0f8d17dbc668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU \n",
    "#use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ec63a-af0f-47f7-85a2-1e313eda9d7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a30b3-d1b1-43c6-bd49-78fdd501d04a",
   "metadata": {},
   "source": [
    "## Load & Prepare Data\n",
    "\n",
    "### Load Vocabulary\n",
    "\n",
    "In the Data Preparation notebook, we already preprocessed and vectorized our corpus of movie reviews, and saved the resulting vocabulary and dataset into files. We essentially only need to load the generated files. Let's start with the vocabulary. Recall, the `vocabulary` is a `vocab` object from the `torchtext` package, allowing us to map words/tokens to their unique integer identifiers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31db184-15ca-4641-8c1e-8b3ef2f4af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:\t20004\n"
     ]
    }
   ],
   "source": [
    "vocabulary = torch.load('data/corpora/imdb-reviews/vectorized-rnn-lm/imdb-rnn-lm-20000.vocab')\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print('Size of vocabulary:\\t{}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4238da90",
   "metadata": {},
   "source": [
    "### Load & Filter Sentences\n",
    "\n",
    "To simplify things, we only consider sentences of a certain length, here from 5..30 words. This is a kind of arbitrary choice, so feel free to change that. However, recall that we already limited ourselves to sentences with lengths between 5 and 50 when generating the dataset in the Date Preparation notebook. So we are bound by this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce41cdf-762e-4c8a-a331-768a3fdd4354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 1085, 4, 489, 9, 4, 7371, 10, 1, 4, 489, 9...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[63, 1, 1642, 10, 35, 6862, 5, 34, 676, 8, 346...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[4, 2449, 1970, 124, 484, 1, 63, 4, 140, 18, 9...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[63, 14, 101, 564, 54, 17, 3024, 5, 17, 14, 26...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[170, 9, 4980, 246, 18, 47, 19, 91, 19, 13766,...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0   1\n",
       "3  [1, 1085, 4, 489, 9, 4, 7371, 10, 1, 4, 489, 9...  30\n",
       "4  [63, 1, 1642, 10, 35, 6862, 5, 34, 676, 8, 346...  17\n",
       "6  [4, 2449, 1970, 124, 484, 1, 63, 4, 140, 18, 9...  30\n",
       "8  [63, 14, 101, 564, 54, 17, 3024, 5, 17, 14, 26...  20\n",
       "9  [170, 9, 4980, 246, 18, 47, 19, 91, 19, 13766,...  16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/corpora/imdb-reviews/vectorized-rnn-lm/imdb-rnn-lm-sentences.txt', header=None)\n",
    "\n",
    "# Keep only sentences with 5 to 30 tokens/words\n",
    "df = df[(df[1] <= 30) & (df[1] >= 5)]\n",
    "\n",
    "# Convert the strings of token indices to list of token indices\n",
    "df[0] = df[0].apply(lambda x: [ int(i) for i in x.split(' ')])\n",
    "\n",
    "# Let's have a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af310b-ed77-455e-9240-8f77e023b795",
   "metadata": {},
   "source": [
    "Recall that the 2nd column in our file records the length of each sequence in terms of the number of token indices. This information will help us in the next step to organize our training batches; see below. Of course, we could also calculate the lengths of the sequence on the fly, but having the value already at hand makes the next step just a bit more convenient.\n",
    "\n",
    "Let's quickly check the total number of sentences. If you used the default settings in the Data Preparation notebook and considered all reviews -- and not just a subset -- the dataset should include 961,348 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfa2cd39-01ec-48a4-8fba-d3835303de50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentencens: 961348\n"
     ]
    }
   ],
   "source": [
    "print('Total number of sentencens: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb2446",
   "metadata": {},
   "source": [
    "### Organize Sentences by Length\n",
    "\n",
    "In the Sentiment Analysis notebook, we already discussed the need to handle sequences of variable length to support mini batches when training our model. Again, we use this approach to organize our dataset in such a way that we can create a batch where all samples are guaranteed to have the same length. However, to show even this approach can be implemented in different ways, we use a more \"manual\" approach in this notebook To this end, we create a dictionary with all sentence lengths as the keys, and where the value for a key is the list of sentences with that length. This will allow us during training to create batches where all samples have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b58cfd6-28bb-4264-8974-5c0de23b205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 961348/961348 [02:07<00:00, 7560.90it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_mapping = {}\n",
    "\n",
    "for i in tqdm(range(0, len(df))):\n",
    "    sent_len = df.iloc[i][1]\n",
    "    if sent_len in sentence_mapping:\n",
    "        sentence_mapping[sent_len].append(i)\n",
    "    else:\n",
    "        sentence_mapping[sent_len] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "205309a7-e538-4702-8316-3e4e25021178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of unique sentence lengths across the whole dataset\n",
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n"
     ]
    }
   ],
   "source": [
    "sent_lengths = df[1].unique()\n",
    "\n",
    "print('List of unique sentence lengths across the whole dataset')\n",
    "print(sorted(sent_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0cc7dc-2371-4a58-9f9c-b364af442a4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b969f4f",
   "metadata": {},
   "source": [
    "## Training a Vanilla RNN Language Model\n",
    "\n",
    "In this notebook, we will train 2 language models using 2 different implementations of an RNN-based network architecture. Fundamentally, both network will implement the exact same train task of learning a language model as described and visualized on the lecture slides; see the figure below:\n",
    "\n",
    "<img src='data/images/lecture-slide-03.png' width='90%' />\n",
    "\n",
    "The network will implement this network essentially 1:1 by reducing the architecture to the most basic components: using the most basic function to update the hidden state in each iteration and using only a single linear layer at the end as the output layer. The figure below again is taken from the lecture slides to visualize the basic computation steps behind the Vanilla RNN network.\n",
    "\n",
    "<img src='data/images/lecture-slide-04.png' width='90%' />\n",
    "\n",
    "\n",
    "### Create Model\n",
    "\n",
    "You can check the file `src/rnn.py` for the implementation of class `VanillaRnnLanguageModel`. It should be relatively easy to match the defined layers in the `__init__()` method an the processing steps in the `forward()` method to the image above. We have 2 parameters to specify: the dimension of the word embeddings (i.e., `embed_size`), and the dimension of the hidden state (i.e., `hidden_size`). As usual, feel free to change `embed_size` and `hidden_size` and see how it affects the quality of the resulting language model. As we don't use any pretrained word embeddings in this notebook -- but we could, of course, and feel free to extend the notebook accordingly -- you can also pick a embedding size outside 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374b7fa-cd1b-4f70-8e5b-f078140f8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, hidden_size = 300, 512\n",
    "# Create model\n",
    "vanilla_rnn_lm_model = VanillaRnnLanguageModel(vocab_size, embed_size, hidden_size).to(device)\n",
    "# Define optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define loss function\n",
    "optimizer = optim.Adam(vanilla_rnn_lm_model.parameters(), lr=0.001)\n",
    "# Print model\n",
    "print(vanilla_rnn_lm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e61cab",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "The code cell below contains the training loop. As usual, we have a nested loop: the outer loop for the epochs and the inner loop for the batches. In the previous image you already saw that the input sequence and the target sequence for a training sample are almost the some, except:\n",
    "\n",
    "* The target sequence is shifted to the left by 1 time step or token\n",
    "\n",
    "* An end-of-sequence (here: `<EOS>`) token is added to the end of the target sequences\n",
    "\n",
    "* An start-of-sequence (here: `<SOS>`) token is added to the end of the input sequences\n",
    "\n",
    "**Important:** Of course, the chosen start-of-sequence and end-of-sequence must match the one defined as special tokens when generating the vocabulary; see the Data Preparation notebook.\n",
    "\n",
    "If you want to continue training after the code cell finished, you can simply run the code cell below again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ebe55-d776-43e5-a6ac-b4072dcd3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Shuffle the list of sentence lengths (good practice)\n",
    "    np.random.shuffle(sent_lengths)\n",
    "    \n",
    "    with tqdm(total=df.shape[0]) as progress_bar:\n",
    "        # Iterate over all possible sentence lengths\n",
    "        for sent_len in sent_lengths:\n",
    "\n",
    "            # The the indices of all sentences of length sent_len\n",
    "            sent_indices = sentence_mapping[sent_len]\n",
    "            sent_indices = np.array(sent_indices)\n",
    "\n",
    "            # Shuffle array of sentence indices (good practice)\n",
    "            np.random.shuffle(sent_indices)\n",
    "\n",
    "            # Compute the number of batches\n",
    "            num_batches = int(np.ceil(len(sent_indices) / batch_size))\n",
    "\n",
    "            # Loop over all possible batches\n",
    "            for batch_indices in np.array_split(sent_indices, num_batches):\n",
    "\n",
    "                # Get sentence data based on the indices in the batch\n",
    "                targets = np.array(df.iloc[batch_indices][0].to_list())\n",
    "\n",
    "                # Since we build language model, inputs and targets are (almost) same\n",
    "                # we only need to shift the target sequence one step to the left which we do below\n",
    "                inputs = targets[:]\n",
    "\n",
    "                # Add SOS token to all input sequences; add EOS token to all target sequences\n",
    "                sos = np.array([vocabulary.lookup_indices(['<SOS>'])]*(len(batch_indices))).reshape(1, -1).T\n",
    "                eos = np.array([vocabulary.lookup_indices(['<EOS>'])]*(len(batch_indices))).reshape(1, -1).T            \n",
    "                targets = np.hstack((targets, eos))\n",
    "                inputs = np.hstack((sos ,inputs))\n",
    "\n",
    "                # Convert data to tensor and move to GPU (if available)\n",
    "                inputs = torch.Tensor(inputs).long().to(device)\n",
    "                targets = torch.Tensor(targets).long().to(device)\n",
    "\n",
    "                # Initialize the first hidden state h0\n",
    "                hidden = vanilla_rnn_lm_model.init_hidden(len(batch_indices)).to(device)\n",
    "                \n",
    "                loss = 0\n",
    "                for i in range(inputs.shape[1]):\n",
    "                    output, hidden = vanilla_rnn_lm_model(inputs[:,i], hidden)\n",
    "                    l = criterion(output, targets[:,i])\n",
    "                    loss += l\n",
    "\n",
    "                # Let PyTorch do its magic to calculate the gradients and update all trainable parameters                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Keep track of overall epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.update(len(batch_indices))\n",
    "\n",
    "    print('[Epoch {}] Loss: {}'.format((epoch+1), epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74e564",
   "metadata": {},
   "source": [
    "### Save/Load Model\n",
    "\n",
    "As retraining the model all the time can be tedious, we can save and load our model. If you trained the model for the first time, only saving is of course possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c616c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 'save'\n",
    "#action = 'load'\n",
    "#action = 'none'\n",
    "\n",
    "if action == 'save':\n",
    "    torch.save(vanilla_rnn_lm_model.state_dict(), 'data/models/language-model/imdb-vanilla-rnn-lm.pt')\n",
    "elif action == 'load':\n",
    "    model = VanillaRnnLanguageModel(vocab_size, embed_size, hidden_size).to(device)\n",
    "    model.load_state_dict(torch.load('data/models/language-model/imdb-vanilla-rnn-lm.pt'))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edcac3",
   "metadata": {},
   "source": [
    "### Generate Sentence Using the Language Model\n",
    "\n",
    "Most basically, the generation of new sentences is quite similar to the training loop. If we have a set of seed words, we first feed those into the model. After that, we use the last predicted word to be the input for the next iteration and so on. The image below is taken from the lecture slides and visualizes the idea of generating sentences using a trained language model\n",
    "\n",
    "<img src='data/images/lecture-slide-05.png' width='90%' />\n",
    "\n",
    "We stop when we predict the EOS token -- or when we reach the maximum length `max_len`. The method `generate()` below implements this idea. You can check the implementation of the `forward()` method of the class `VanillaRnnLanguageModel` to see the similarity of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, seed_tokens, max_len=30):\n",
    "    # Create initial input as SOS token + seed words\n",
    "    inputs = np.array(vocabulary.lookup_indices(['<SOS>']) + vocabulary.lookup_indices(seed_tokens))\n",
    "\n",
    "    # Convert input to tensor and move to GPU (if available)\n",
    "    inputs = torch.Tensor(inputs).long().unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize the first hidden state h0\n",
    "    hidden = model.init_hidden(1).to(device)\n",
    "    \n",
    "    # Keep track of the predicted word which forms our final result\n",
    "    tokens = seed_tokens\n",
    "    \n",
    "    # Push SOS token (and optional seed words through the RNN)\n",
    "    for i in range(inputs.shape[1]):\n",
    "        logits, hidden = model(inputs[:,i], hidden)\n",
    "    \n",
    "\n",
    "    # Iterate over the time steps to predict the next word step by step\n",
    "    for k in range(max_len):\n",
    "        \n",
    "        # Get index of word with the highest probability (no sampling here to keep it simple)\n",
    "        _, topi = logits[-1].topk(1)\n",
    "        word_index = topi.item()\n",
    "\n",
    "        # If we predict the EOS token, we can stop\n",
    "        if word_index == vocabulary.lookup_indices(['<EOS>'])[0]:\n",
    "            break\n",
    "\n",
    "        # Get the respective word/token and add it to the result list\n",
    "        #tokens.append(index2word[word_index])\n",
    "        tokens.append(vocabulary.lookup_token(word_index))\n",
    "            \n",
    "        # Create the tensor for the last predicted word\n",
    "        next_input = torch.tensor([word_index]).to(device)\n",
    "        \n",
    "        # Use last predicted word as input for the next iteration\n",
    "        logits, hidden = model(next_input, hidden)\n",
    "      \n",
    "    # Return the result words/tokens as a string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18ba98",
   "metadata": {},
   "source": [
    "### Generating some Example Sentences\n",
    "\n",
    "The example seed words are the ones to generate the lecture slide. Of course, the generated sentence will greatly depend on how long and with which hyperparameter values the language model has been trained. You can of course add your own examples or modify the existing ones. You are also encouraged to try the same example after multiple rounds of training. Particularly if you train the model for only very few epochs, you are very likely to see mostly *\"funny\"* sentences -- that is, sentences that might be grammatically mostly fine but don't really make much sense...or no sense at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b37016-41d7-47d6-bf3f-62e06be6a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(vanilla_rnn_lm_model, ['the', 'cast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(vanilla_rnn_lm_model, ['i', 'love', 'how']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(vanilla_rnn_lm_model, ['my', 'dad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(vanilla_rnn_lm_model, ['this', 'was']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f99a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(vanilla_rnn_lm_model, ['some', 'of', 'the']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58208d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(vanilla_rnn_lm_model, ['the', 'script']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ece79-5d45-4c67-b251-76a7d0026669",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa15f7-9da0-4254-ad70-8192a2eccdb1",
   "metadata": {},
   "source": [
    "## Training an (a bit more) Advanced RNN Language Model\n",
    "\n",
    "The Vanilla RNN Model was limited to the basics in terms of the function to update the hidden state and the use of additional hidden linear layers -- well it had none of those. We already know the LSTMs and GRUs offer generally better approaches to update the hidden state, so we should make use of it. Thus, in the following, we will build and train a slightly more advanced RNN-based language model. Check out the implementation of the class `RnnLanguageModel` in the file `src/rnn.py`.\n",
    "\n",
    "### Create Model\n",
    "\n",
    "Similar to the class `RnnTextClassifier`, the class `RnnLanguageModel` is implemented in a rather flexible manner to easily change the network architecture by changing a series of input parameters. In fact -- and this shouldn't be a surprise -- the set of input parameters is very similar to the one of `RnnTextClassifier`. With the right parameter setting, we can also mimic the Vanilla RNN Model from above; see the code cell below for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc3f32bf-3f4a-4b63-9f9b-1589b0174ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RnnLanguageModel(\n",
      "  (embedding): Embedding(20004, 300)\n",
      "  (rnn): GRU(300, 512, batch_first=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (out): Linear(in_features=1024, out_features=20004, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"device\": device,                   # as the class also generates sentence it mus be able to move the data to the correct device\n",
    "    \"vocab_size\": vocab_size,           # the size of the vocabulary determines the input size of the embedding\n",
    "    \"embed_size\": 300,                  # size of the word embeddings\n",
    "    \"rnn_cell\": \"GRU\",                  # in practice GRU or LSTM will always outperform RNN\n",
    "    \"rnn_num_layers\": 1,                # 1 or 2 layers are most common; more rarely sees any benefit\n",
    "    \"rnn_hidden_size\": 512,             # size of the hidden state\n",
    "    \"rnn_dropout\": 0.0,                 # only relevant if rnn_num_layers > 1\n",
    "    \"linear_hidden_sizes\": [1024],      # list of sizes of subsequent hidden layers; can be [] (empty)!\n",
    "    \"linear_dropout\": 0.5,              # if hidden linear layers are used, we can also include Dropout\n",
    "}\n",
    "\n",
    "##\n",
    "## Side note: the parameter setting below essentially correspond to the Vanilla RNN Model\n",
    "##\n",
    "#params = {\n",
    "#    \"device\": device,\n",
    "#    \"vocab_size\": vocab_size,\n",
    "#    \"embed_size\": 300,\n",
    "#    \"rnn_cell\": \"RNN\",\n",
    "#    \"rnn_num_layers\": 1,\n",
    "#    \"rnn_hidden_size\": 512,\n",
    "#    \"rnn_dropout\": 0.0,\n",
    "#    \"linear_hidden_sizes\": [],\n",
    "#    \"linear_dropout\": 0.0,\n",
    "#}\n",
    "\n",
    "\n",
    "# Define model paramaters\n",
    "params = Dict2Class(params)\n",
    "# Define model\n",
    "rnn_lm_model = RnnLanguageModel(params).to(device)\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(rnn_lm_model.parameters(), lr=0.001)\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Print model\n",
    "print(rnn_lm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03ccee-21e8-4139-b415-5789681aa4cb",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "The training loop in the code cell below is almost identical to the one above, particularly the parts preparing the input and target sequences. However, running the model and computing the loss is a bit different due to the implementation and therefore use of the class `RnnLanguageModel`. As before, if you want to continue training after the code cell finished, you can simply run the code cell below again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f2ec7-2e70-4790-abdc-54e04ce0edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 961348/961348 [05:58<00:00, 2683.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 34369.55098962784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████████████████████████████████▌                               | 610058/961348 [03:44<02:18, 2539.06it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Shuffle the list of sentence lengths (good practice)\n",
    "    np.random.shuffle(sent_lengths)\n",
    "    \n",
    "    with tqdm(total=df.shape[0]) as progress_bar:\n",
    "        # Iterate over all possible sentence lengths\n",
    "        for sent_len in sent_lengths:\n",
    "\n",
    "            # The the indices of all sentences of length sent_len\n",
    "            sent_indices = sentence_mapping[sent_len]\n",
    "            sent_indices = np.array(sent_indices)\n",
    "\n",
    "            # Shuffle array of sentence indices (good practice)\n",
    "            np.random.shuffle(sent_indices)\n",
    "\n",
    "            # Compute the number of batches\n",
    "            num_batches = int(np.ceil(len(sent_indices) / batch_size))\n",
    "\n",
    "            # Loop over all possible batches\n",
    "            for batch_indices in np.array_split(sent_indices, num_batches):\n",
    "\n",
    "                # Get sentence data based on the indices in the batch\n",
    "                targets = np.array(df.iloc[batch_indices][0].to_list())\n",
    "\n",
    "                # Since we build language model, inputs and targets are (almost) same\n",
    "                # we only need to shift the target sequence one step to the left which we do below\n",
    "                inputs = targets[:]\n",
    "\n",
    "                # Add SOS token to the FRONT of all input sequences; add EOS token to the END of all target sequences\n",
    "                sos = np.array([vocabulary.lookup_indices(['<SOS>'])]*(len(batch_indices))).reshape(1, -1).T\n",
    "                eos = np.array([vocabulary.lookup_indices(['<EOS>'])]*(len(batch_indices))).reshape(1, -1).T            \n",
    "                targets = np.hstack((targets, eos))\n",
    "                inputs = np.hstack((sos ,inputs))\n",
    "\n",
    "                # Convert data to tensor and move to GPU (if available)\n",
    "                inputs = torch.Tensor(inputs).long().to(device)\n",
    "                targets = torch.Tensor(targets).long().to(device)\n",
    "        \n",
    "                # Initialize hidden states w.r.t. batch size (batches might not always been full)\n",
    "                hidden = rnn_lm_model.init_hidden(inputs.shape[0]) \n",
    "        \n",
    "                outputs, _ = rnn_lm_model(inputs, hidden)\n",
    "                \n",
    "                loss = criterion(outputs.permute(0,2,1), targets)\n",
    "\n",
    "                ### Pytorch magic! ###\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Keep track of overall epoch loss\n",
    "                epoch_loss += loss.item()                \n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.update(len(batch_indices))\n",
    "    \n",
    "    print('[Epoch {}] Loss: {}'.format((epoch+1), epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a177d8-291b-49ef-911f-388ecc343e0d",
   "metadata": {},
   "source": [
    "### Save/Load Model\n",
    "\n",
    "With the code cell below, we can also save and load this more advanced language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98a893-d0dd-440b-b8de-e9afe9149b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 'save'\n",
    "#action = 'load'\n",
    "#action = 'none'\n",
    "\n",
    "if action == 'save':\n",
    "    torch.save(rnn_lm_model.state_dict(), 'data/models/language-model/imdb-rnn-lm.pt')\n",
    "elif action == 'load':\n",
    "    rnn_lm_model = RnnLanguageModel(params).to(device)\n",
    "    rnn_lm_model.load_state_dict(torch.load('data/models/language-model/imdb-rnn-lm.pt'))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a419aeb-bcc6-426a-abca-79f4d79276e2",
   "metadata": {},
   "source": [
    "### Generate Sentence Using the Language Model\n",
    "\n",
    "If you had a look at the implementation of the class `RnnLanguageModel`, you will have noticed that it already includes a `generate()` method to generate sentences based on a given set of seed words. This means we can now simply call this class method to again look at some example sentences our model will generate. The code cell below uses the same seed token as above, but you can always add or modify the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a4517-7624-4704-98e4-a6850faae355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_lm_model.generate(['the', 'cast'], vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a6dea-c326-4ff0-bb6b-1242c0b57606",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_lm_model.generate(['i', 'love', 'how'], vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab0ab2-a93c-4e70-ad5d-76bd2ba32ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_lm_model.generate(['my', 'dad'], vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623f30b-4f12-4abb-b6fd-c29a2c2438b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_lm_model.generate(['this', 'was'], vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebee47-19f2-4662-8c7c-d68f19d89196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_lm_model.generate(['some', 'of', 'the'], vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd1606-11b8-4174-bc69-e3b6b011ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_lm_model.generate(['the', 'script'], vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4583d-2d32-442e-9598-58f4a09addc3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87b2fc-af4e-48b1-9c16-8743016ae8be",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "This notebook -- together with the Data Preparation notebook -- has shown that building and training an RNN-based language model is actually not that difficult. However, we have made several simplifying assumptions to focus on the core steps without the goal to train a competitive language model. Here are a couple of points to consider when trying to build more practical, large(r)-scale language models:\n",
    "\n",
    "* Using the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to train a language model has of course its limitations. Firstly, the dataset is very small for this task, and maybe more importantly using a dataset from a specific domain (i.e.: movie reviews) limits its applicability in this domain. However, the focus here is to go through some of the basic steps and not to build a state-of-the-art language model.\n",
    "\n",
    "* The goal in this notebook was to build a simple language model to generate single sentences, not paragraphs or even beyond. Hence each data sample we generate will reflect a single sentence. For training very large language models to generate paragraphs, samples are typically chunks of text containing multiple sentences that might even be arbitrarily be cut off.\n",
    "\n",
    "* The dataset used in this notebook was small enough that we could easily load it into the main memory. However, datasets to build proper language models are huge and would not fit into the memory all at once. In this case, some logic is required to first split the whole dataset into multiple chunks (e.g., different files) and then iterate over all chunks within each epoch.\n",
    "\n",
    "* In practice, large language models are typically trained in a distributed setting such as computing clusters housing many CPUs. Deep learning frameworks such as PyTorch and Tensorflow support distributed training and inferencing out of the box, but again, some additional logic is required to facilitate this.\n",
    "\n",
    "* You might have already noticed that using the same seed words will always generate the same sentence for the same model. The reason is that the `generate()` methods of the 2 implementations will always pick as the next word/token the one with the highest probability. To add some variety, we could change the methods to randomly sample a word/token, for example, from the 10 tokens with the highest probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d7dbdc-afb3-4a27-92c5-5323b45b7217",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc81eb-8e85-4a48-8e40-b1e78b761bb6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Recurrent Neural Networks (RNNs) have proven to be highly effective for training language models. RNNs are particularly well-suited for language modeling due to their ability to handle sequential data. Language is inherently sequential, and RNNs can capture the dependencies and context within text by maintaining an internal hidden state that evolves as it processes each word or character in a sequence.\n",
    "\n",
    "When training language models with RNNs, the models are typically fed with a sequence of input tokens, such as words or characters. The RNN processes the input sequence step by step, updating its hidden state at each time step. The hidden state acts as a memory, capturing information about the preceding context. The RNN predicts the next word or character in the sequence based on the current input and the previous hidden state. The model is trained by comparing its predicted output with the actual next element in the sequence, and the gradients of the loss function are backpropagated through time to update the model's parameters.\n",
    "\n",
    "By leveraging the recurrent nature of RNNs, these models can capture both short-term and long-term dependencies in language. This allows them to generate coherent and contextually appropriate text, as well as understand and process complex linguistic structures. RNN-based language models have been successfully applied to various tasks, such as text generation, machine translation, sentiment analysis, and speech recognition. Furthermore, variants of RNNs, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have been introduced to address some of the challenges in training RNNs, such as the vanishing/exploding gradient problem. These advancements have further improved the effectiveness of RNNs for training language models, making them a cornerstone of natural language processing research and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9d6b5-e903-42cb-9f13-ac64813f0908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
