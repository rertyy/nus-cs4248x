{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** This notebook is adopted from the official PyTorch Tutorial [Language Translation with nn.Transformer and torchtext](https://pytorch.org/tutorials/beginner/translation_transformer.html). Overall, the code remains almost unchanged but there are some very minor modifications:\n",
    "\n",
    "* The code has been re-organized to have all import statements at the beginning of the notebook.\n",
    "\n",
    "* Some minor updates have been made to avoid some warning messages.\n",
    "\n",
    "* Some variable names and values have been modified to be inline with previous course notebooks; the training and evaluation loops come now with progress bars to see that something is happening.\n",
    "\n",
    "The main difference between this notebook and the official PyTorch tutorial is in the added explanations and discussion of the code. This should make it easier to understand and follow all the performed steps in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Translation with `nn.Transformer` and `torchtext`\n",
    "\n",
    "In a previous notebook, we addressed the task of machine translation using an RNN-based encoder-decoder architecture. Now we will see how Transformers can be used for this task. Transformers have revolutionized the field of machine translation in NLP and offer several advantages over traditional approaches. Here are some key advantages of Transformers for machine translation:\n",
    "\n",
    "* **Attention Mechanism:** Transformers employ a self-attention mechanism that allows them to capture dependencies between words in a sentence effectively. Unlike traditional RNNs that process input sequentially, Transformers can attend to all words simultaneously, enabling them to capture long-range dependencies more efficiently. This attention mechanism helps the model understand the context of each word in relation to the entire input sequence, which is crucial for accurate translation.\n",
    "\n",
    "* **Parallelization:** Transformers are highly parallelizable, meaning they can process multiple input sentences concurrently. This parallelization capability is particularly advantageous for machine translation tasks since it speeds up the training and inference process. In contrast, traditional recurrent models process sentences sequentially, leading to slower computation times.\n",
    "\n",
    "* **Contextual Representation:** Transformers excel at capturing contextual information. They generate contextualized word representations, allowing them to consider the meaning of a word within the context of the entire sentence. This contextual representation facilitates more accurate translations by enabling the model to understand subtle linguistic nuances, idiomatic expressions, and word sense disambiguation.\n",
    "\n",
    "* **Bidirectional Encoding:** Transformers employ bidirectional encoding, which means they consider both the left and right context of a word during the encoding process. This bidirectional approach allows the model to capture dependencies between words in both directions, resulting in a more comprehensive understanding of the input sentence. It helps address the limitations of traditional models that process sentences only in one direction, such as RNNs.\n",
    "\n",
    "* **Transfer Learning:** Transformers can be effectively pretrained on large-scale corpora and then fine-tuned on specific machine translation tasks. This transfer learning approach allows the model to leverage the knowledge acquired during pretraining, such as understanding syntax, semantics, and general language structure. Fine-tuning on translation data further enhances the model's ability to generate accurate and coherent translations.\n",
    "\n",
    "* **Long-Term Dependency Handling:** Transformers mitigate the vanishing gradient problem that affects traditional recurrent models like RNNs. RNNs struggle to capture long-term dependencies due to the decay of gradients over time. Transformers, on the other hand, use self-attention to directly connect any two words in a sentence, allowing them to model long-range dependencies effectively and capture essential information from distant words.\n",
    "\n",
    "These advantages of Transformers make them highly suitable for machine translation tasks in NLP, enabling them to achieve state-of-the-art performance and improve translation quality compared to previous approaches. Of course, this assumes training over very large datasets which is beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable, List\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize some utility methods from PyTorch as well as Torchtext, so we need to import the `torch` and `torchtext` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking/Setting the Computation Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPUs to significantly speed up the training process. If you have a support GPU, feel free to utilize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU \n",
    "#use_cuda = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Requirements\n",
    "\n",
    "Although we do not explicitly import spaCy, The `get_tokenizer()` method of `torchtext` will use the spaCy tokenizer. This means you need to have spaCy installed together with the language models used in this notebook. Below are the `pip` commands to install spaCy and the required language models, if needed. If you use `conda` or `mamba` as you Python package manager, the command for installing spaCy will differ:\n",
    "\n",
    "```python\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "We will also make use of a dataset provided by `torchtext`. Thus, you need to ensure to all install the `torchdata` package.\n",
    "\n",
    "```python\n",
    "pip install -U torchdata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sourcing and Processing\n",
    "\n",
    "The [`torchtext`](https://pytorch.org/text/stable/) has utilities for creating datasets that can be easily iterated through for the purposes of creating a language translation model. In this example, we show how to use `torchtext`'s inbuilt datasets, tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensors. We will use [Multi30k](https://pytorch.org/text/stable/datasets.html#multi30k) dataset from torchtext library that yields a pair of source-target raw sentences. The Multi30k dataset is a popular benchmark dataset used in the field of machine translation and multimodal research. It is specifically designed for the task of translating natural language descriptions to corresponding images. The dataset contains sentence-image pairs where the sentences describe the content of the images. Here are the key features of the Multi30k dataset:\n",
    "\n",
    "* **Size:** The dataset consists of approximately 30,000 sentence-image pairs. It is a relatively large-scale dataset, providing ample data for training and evaluating machine translation models.\n",
    "\n",
    "* **Multilingual:** The Multi30k dataset includes parallel descriptions in multiple languages, making it useful for multilingual machine translation research. The original dataset contains English-German pairs, but it has been expanded to include other languages such as French and Czech.\n",
    "\n",
    "* **Image Content:** The images in the dataset are sourced from the Flickr30k dataset, which contains 31,014 images. Each image is paired with five English descriptions and their translations in other languages. The descriptions vary in length and complexity, ranging from simple to more detailed and nuanced sentences.\n",
    "\n",
    "* **Training, Validation, and Test Sets:** The Multi30k dataset is divided into training, validation, and test sets. The training set typically comprises the majority of the dataset and is used for training machine translation models. The validation set is used to tune hyperparameters and make early stopping decisions during training. The test set serves as an unbiased evaluation set to measure the performance of trained models.\n",
    "\n",
    "* **Annotations:** The Multi30k dataset includes annotations for each sentence-image pair, providing additional information such as part-of-speech tags and parse trees. These annotations can be used to enhance the training process or explore linguistic aspects of the data.\n",
    "\n",
    "The Multi30k dataset has been widely used by researchers to develop and evaluate machine translation models, especially those incorporating image and text modalities. It facilitates research in multimodal translation, where the goal is to generate accurate and meaningful translations by leveraging both visual and textual information. In this notebook, we will only use the textual information and ignore any images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Placeholders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Methods\n",
    "\n",
    "We use the spaCy tokenizer to tokenize all sentences. For this, we use the `get_tokenizer()` method of `torchext` to load the tokenizer for the source language (German) and the target language (English). We store both tokenizers in the `token_transform` and index the respective tokenizer using the language identifiers `'de'` and `'en'`. We also define a generator method `yield_tokens()` which takes an iterator over the sentences as input and returns the tokenized sentence as output by call the respective tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# Helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabularies\n",
    "\n",
    "We can now build the vocabularies for the source and target language. As usual, we first need to define a set of special tokens. Note that here we manually specify the indices of all special tokens. Strictly speaking, this step is not required since we could always get these values later using, e.g., `vocab_transform['en']['<PAD>']`. However, this approach makes the code a bit cleaner.\n",
    "\n",
    "**Important:** You need to ensure that the indices of the special tokens match the order of the tokens in list `special_symbols`; so best do not edit the code cell below. This is to ensure that, for example, `vocab_transform['en']['<PAD>'] == PAD_IDX` is indeed true.\n",
    "\n",
    "`torchtext` dataset returns an iterator making it quite easy to create the vocabularies using the in-built method `build_vocab_from_iterator()` -- in previous notebooks, we did this step more \"manually\" by using a `Counter` and `OrderedDict`, etc. If you check the code below, you will notice that we actually iterate twice over the same data iter `train_iter`, once for each language. This is of course not very efficient, but since the dataset is rather small, we use the convenient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "PAD_IDX, UNK_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set `UNK_IDX` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** In previous notebooks, we considered the building of the vocabularies and the vectorization a separate step -- that is, we built the vocabularies, vectorized the sentences and saved the result in files for later use. Note that here, we only build the two vocabularies. The reason is that in this notebook, we will vectorize our batches of sentence pairs \"on the fly\" during each epoch (we will define more auxiliary methods for this later). Vectorizing each batch during training will naturally add some overhead to the process which will result in some longer training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Network using Transformer\n",
    "\n",
    "### Auxiliary Layers\n",
    "\n",
    "In the notebook covering the basic Transformer architecture, we also discusses the importance of a **Positional Encoding** layer to preserve the information of the word order. While we implemented a `PositionalEncoding` class in the file `src/transformer.py`, we use the implementation of this class below to adhere to the original PyTorch tutorial, and to keep this notebook self-contained. However, you should compare both implementations to convince yourself that they essentially perform exactly the same steps.\n",
    "\n",
    "The implementation of the model also uses the `TokenEmbedding` layer. If you check below, this layer merely wraps an `nn.Embedding` layer we already used multiple times. The only difference/addition is that the result embeddings are scaled with respect to the embedding size. Scaling word embeddings with respect to the embedding size is a technique commonly used in natural language processing (NLP) tasks. The purpose of scaling word embeddings is to ensure that the embeddings have a consistent magnitude across different embedding dimensions. This scaling helps in normalizing the embeddings and can lead to improved model performance. Here's why scaling is beneficial:\n",
    "\n",
    "* **Avoiding Bias Towards Certain Dimensions:** Word embeddings are typically represented as vectors in a high-dimensional space, where each dimension captures a different aspect of the word's meaning. However, the magnitudes of these dimensions can vary widely. By scaling the embeddings, you ensure that no dimension dominates over others, thereby avoiding bias towards specific dimensions. This prevents the model from assigning disproportionate importance to certain aspects of word meanings.\n",
    "\n",
    "* **Gradient Stability:** Scaling word embeddings can help with gradient stability during training. When the magnitudes of different embedding dimensions differ significantly, it can lead to gradients with varying scales. Large gradients in some dimensions and small gradients in others can make the optimization process challenging and slow down training. Scaling the embeddings helps balance the gradient scales, making the optimization more stable and efficient.\n",
    "\n",
    "* **Similarity Measures:** In various NLP tasks, such as measuring semantic similarity or calculating distances between word embeddings, scaling helps ensure consistent and meaningful comparisons across dimensions. Without scaling, the magnitude differences can distort the similarity measures, leading to inaccurate results. Scaling ensures that the distance or similarity between word embeddings is based on meaningful comparisons across all dimensions.\n",
    "\n",
    "* **Regularization:** Scaling word embeddings can act as a form of regularization. By enforcing a consistent magnitude across embedding dimensions, it can prevent individual dimensions from having excessive influence on the overall embedding representation. This regularization can improve the generalization capability of the model, reducing overfitting and improving its ability to handle unseen data.\n",
    "\n",
    "It's worth noting that scaling word embeddings with respect to the embedding size is just one scaling technique among several possibilities. Other scaling methods, such as unit norm scaling or custom scaling factors, may also be used depending on the specific requirements of the task or the characteristics of the embeddings. The goal is to ensure that the word embeddings are appropriately scaled to promote better training, more meaningful comparisons, and improved model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Architecture\n",
    "\n",
    "We implemented the Transformer architecture from scratch in the previous notebook. However, for an optimal performance, we will use an available Transformer implementation as the core component of our model. In PyTorch, the `nn.Transformer` module is an implementation of the Transformer model. The module provides a high-level interface for creating and training Transformer models in PyTorch. It encapsulates the core components of the Transformer architecture, including the encoder, decoder, and attention mechanisms. Here are the key components of the `nn.Transformer` module:\n",
    "\n",
    "* **Encoder and Decoder:** The Transformer model consists of an encoder and a decoder. The encoder takes an input sequence and processes it to capture the contextual representations of the input tokens. The decoder takes the encoder's outputs and generates the output sequence step by step.\n",
    "\n",
    "* **Attention Mechanism:** The Transformer's attention mechanism is a fundamental component that allows the model to capture dependencies between tokens efficiently. The nn.Transformer module incorporates multi-head self-attention, enabling the model to attend to different parts of the input sequence simultaneously.\n",
    "\n",
    "* **Feed-Forward Networks:** The Transformer model includes feed-forward neural networks within the encoder and decoder. These networks provide non-linear transformations to the attention outputs, allowing the model to capture complex patterns and relationships between tokens.\n",
    "\n",
    "* **Masking:** The nn.Transformer module supports masking capabilities, particularly for the decoder. Masking is crucial to prevent the model from attending to future tokens during training, ensuring the model's autoregressive property.\n",
    "\n",
    "The code cell below implements the class `Seq2SeqTransformer` as our final model. Apart from the `nn.Transformer` layer, it naturally also includes the `PositionalEncding` layer and `TokenEmbedding` layer from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "In the context of Transformers, masking refers to a technique used to control the flow of information during the self-attention mechanism within the model. Transformers rely on self-attention to capture the relationships between different words or tokens in a sequence. Masking is particularly important in tasks where the model processes sequences, such as machine translation, language modeling, or text classification.\n",
    "\n",
    "There are two commonly used types of masks in Transformers: padding masks and lookahead masks.\n",
    "\n",
    "* **Padding masks:** When processing a batch of sequences, they may have different lengths. To handle variable-length sequences efficiently, padding is often used to make all sequences in a batch the same length by adding special tokens like <PAD>. Padding masks are used to mask out the padded positions during self-attention calculations. This ensures that the model does not attend to the padded positions, which do not contain meaningful information.\n",
    "\n",
    "* **Lookahead masks:** In language modeling or autoregressive tasks (incl. machine translation), where the model predicts the next token given the previous tokens, a lookahead mask is applied to ensure that each token can only attend to the previous tokens and not to the tokens that follow it. This prevents the model from cheating by looking ahead at tokens it should not have access to during training or generation.\n",
    "\n",
    "On an implementation level mask are matrices/tensors whose values indicate to which token to attend to or not. These values can commonly be Booleans (`True` or `False`) or Integers (`0` or `1`, sometimes also `0` or `-inf`). The choice of values depends on the exact implementation of the transformer. For example `nn.Transformer` supports `0/1` as well as `True/False` masks. Let's first look at an example for a padding mask using boolean values. Suppose we have a batch of four sequences with varying lengths but padded with `0` to enforce the same length:\n",
    "    \n",
    "```\n",
    "Sequence 1: [6, 4, 2, 1, 9]\n",
    "Sequence 2: [5, 3, 4, 0, 0]\n",
    "Sequence 3: [7, 8, 2, 3, 0]\n",
    "Sequence 4: [1, 2, 0, 0, 0]\n",
    "```\n",
    "\n",
    "The corresponding padding mask will then look like:\n",
    "    \n",
    "    \n",
    "```\n",
    "[\n",
    "  [False, False, False, False, False],\n",
    "  [False, False, False, True, True],\n",
    "  [False, False, False, False, True],\n",
    "  [False, False, True, True, True]\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "In this example, the first sequence has a length of 5, so we have 5 `False` values in the first row. The second sequence has a length of 3, so we have 3 `False` values in the second row, and the last two positions are padded, represented by True; and so on for the other 2 sequences. This boolean padding mask can be used in the Transformer network to identify the padded positions and exclude them from attention calculations, ensuring that the model attends only to the relevant tokens in each sequence.\n",
    "    \n",
    "**Important:** In the example above, this will be the padding mask independent from whether the batch is the input of the encoder (i.e., source language) or for decoder (i.e., target language)\n",
    "    \n",
    "Assuming that the batch is the input for the decoder and we are modeling an autoregressive task such as machine translation, we also need a look-ahead mask. For the batch above, the corresponding lookahead mask looks like:\n",
    "    \n",
    "```\n",
    "[\n",
    "  [False,  True,  True,  True,  True,  True],\n",
    "  [False, False,  True,  True,  True,  True],\n",
    "  [False, False, False,  True,  True,  True],\n",
    "  [False, False, False, False,  True,  True],\n",
    "  [False, False, False, False, False,  True],\n",
    "  [False, False, False, False, False, False]\n",
    "]\n",
    "```\n",
    "    \n",
    "In this example, the first row represents the first token, which can attend only to itself, so we have 3 `True` values after the first `False`. The second row represents the second token, which can attend to the first token and itself, so we have 2 `True` values after the second `False`. The third row represents the third token, which can attend to the first, second, and itself, so we have 1 `True` value after the third `False`. The fourth row represents the fourth token, which can attend to all the previous tokens, so we have only `False`. This boolean lookahead mask ensures that each token can only attend to the previous tokens in the sequence and not to the tokens that follow it. It helps maintain the causality constraint in autoregressive tasks, such as language modeling or text generation, where the model predicts the next token based on the previous tokens.\n",
    "    \n",
    "#### Auxiliary Methods    \n",
    "    \n",
    "In the code cell below, the method `create_mask()` -- utilizing the additional method `generate_square_subsequent_mask()` -- generates 4 masks required for the `nn.Transformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask.bool()\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    \n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking: Example\n",
    "\n",
    "To illustrate the purpose of the methods `create_mask()` and `generate_square_subsequent_mask()`, let's go through an example. For this, we assume that the list of 4 example sequences `batch_src` represent the input of the encoder (i.e., source language), and `batch_tgt` represents the input of the decoder (i.e., target language). Note that the sequence length of `batch_src` can differ from the sequence length of `batch_tgt`.\n",
    "\n",
    "**Important:** The implementation of the methods `create_mask()` and `generate_square_subsequent_mask()` assume that the input tensors have a shape of `(sequence_length, batch_size)`. We therefore need to transpose the out 2 tensors using `.T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_src = torch.LongTensor([\n",
    "    [6, 4, 2, 1, 9],\n",
    "    [5, 3, 4, 0, 0],\n",
    "    [7, 8, 2, 3, 0],\n",
    "    [1, 2, 0, 0, 0]\n",
    "]).T  # <-- Transpose!\n",
    "\n",
    "batch_tgt = torch.LongTensor([\n",
    "    [3, 6, 4, 5, 0, 0],\n",
    "    [2, 3, 1, 4, 7, 6],\n",
    "    [5, 4, 1, 2, 5, 3],\n",
    "    [6, 7, 4, 5, 1, 0]\n",
    "]).T  # <-- Transpose!\n",
    "\n",
    "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(batch_src, batch_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at `src_mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see `src_mask` has only `False` values. So in some sense, this mask is not needed here as it represents a lookahead mask but with no restriction. However, there are use cases beyond machine translation where this mask may have some `True` values. For consistency, `nn.Transformer` expects this mask so we give it this one with all `False`.\n",
    "\n",
    "The padding matrix is a bit more interesting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "print(src_padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already discussed above, here we can clearly see that, for example, the second sequence has to padding indices at the end, hence the 2 `True` values at the end of the second row. This tells the encoder of `nn.Transformer` not to attend to the last 2 tokens in the second sequence.\n",
    "\n",
    "For the decoder, `tgt_mask` represents the lookahead mask as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the purpose is to tell the decoder that (a) the first token can only attend to itself, (b) the second token can only attend to the first token and to itself, (c) the third token can only attend to the first/second token and to itself, (d) the fourth token can only attend to the first/second/third token and to itself, (e) ...and so on.\n",
    "\n",
    "Of course, if the sequences for the decoder include padding tokens, we also need the padding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False],\n",
      "        [False, False, False, False, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "print(tgt_padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of `tgt_padding_mask` is the same as of `src_padding_mask`.\n",
    "\n",
    "**Side note:** Padding mask might also have `True` value within a sequence (not only at the end) to indicate which token not to attend to. While this is not the case here for the task of machine translation, masking proper tokens in a sequence is a common technique for Transformer-based language models such [BERT](https://arxiv.org/abs/1810.04805)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collation\n",
    "\n",
    "\n",
    "As seen in the Data Sourcing and Processing section, our data iterator yields a pair of raw strings. We need to convert these string pairs into the batched tensors that can be processed by our `Seq2SeqTransformer` model defined previously. Below we define the `collate_fn()` method that converts a batch of raw strings into batch tensors that can be fed directly into our model.\n",
    "\n",
    "In the context of machine learning, data collation refers to the process of combining and organizing individual data samples or instances into a structured format that can be used for training, validation, or testing of machine learning models. Data collation typically involves gathering data samples from various sources, such as databases, files, or APIs, and transforming them into a suitable representation for model training. This process may include tasks such as data cleaning, preprocessing, feature extraction, and formatting. Key steps involved in data collation include:\n",
    "\n",
    "* **Data collection:** Gathering the raw data from diverse sources, which could be in different formats, such as text, images, or numerical data.\n",
    "\n",
    "* **Data cleaning:** Removing any inconsistencies, errors, or missing values from the collected data. This step ensures the quality and reliability of the data.\n",
    "\n",
    "* **Data preprocessing:** Applying various transformations to the data, such as normalization, scaling, or one-hot encoding, to make it suitable for the machine learning model.\n",
    "\n",
    "* **Feature extraction:** Extracting relevant features from the data to capture important patterns or characteristics that are informative for the learning task.\n",
    "\n",
    "* **Data formatting:** Organizing the data into a structured format, such as matrices or tensors, that can be fed into the machine learning algorithm.\n",
    "\n",
    "* **Splitting into training, validation, and test sets:** Dividing the collated data into separate subsets for model training, model evaluation during development (validation), and final model evaluation (testing).\n",
    "\n",
    "Data collation is a crucial step in the machine learning pipeline, as the quality and structure of the training data can significantly impact the performance and generalization of the learned models. It requires careful consideration of data sources, appropriate preprocessing techniques, and maintaining data integrity throughout the process. Since we take and ready-made dataset and assume that all sentences are well-formed w.r.t. to the source or target language, we only have to consider the following 3 mains steps to prepare our data batches\n",
    "\n",
    "* **Tokenization:** convert raw string into list of tokens\n",
    "\n",
    "* **Vectorization:** transform lists of tokens into list of their corresponding indices (given the built vocabularies)\n",
    "\n",
    "* **Data formatting:** add special tokens `<SOS>` and `<EOS>` at the beginning and end of the token lists\n",
    "\n",
    "The code cell below defines the method `collate_fn` that -- together with additional auxiliary methods -- performs these 3 steps for a given batch of sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Helper method to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# method to add SOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX])))\n",
    "\n",
    "\n",
    "# `src` and `tgt` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], # Tokenization\n",
    "                                               vocab_transform[ln], # Vectorization\n",
    "                                               tensor_transform)    # Add SOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# Method to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "    # pad_sequence is an in-built method proivded by torch package\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Methods for Training and Evaluation\n",
    "\n",
    "Like in previous notebooks, it is useful to define methods that handle the basic training and evaluation loop. This is done using the methods `train_epoch()` and `evaluate()` in the code cell below. Notice that in both methods the `DataLoader` class receives the `collate_fn()` as an input parameter. This ensures that during each iteration, the batch of raw sentence pairs is tokenized, vectorized, and formatted as described above.\n",
    "\n",
    "One implementation detail is worth pointing out. After a batch of raw sentence pairs has been processed by the method `collate_fn()` all sequences in the batch -- for both the source and target language have the following format, using proper tokens instead of indices for better visualization:\n",
    "\n",
    "```\n",
    "[<SOS>, I , went, home, <EOS>, <PAD>, <PAD>, <PAD>, <PAD>]\n",
    "```\n",
    "\n",
    "However, the expected format is as follows:\n",
    "\n",
    "<img src='data/images/transformer-mt-tensor-format.png' width='80%' />\n",
    "\n",
    "While the format of the tensor matches the expected input of the encoder, we need some additional steps to get the right input and output format for the decoder. Most fundamentally, the output of the decoder needs to be shifted 1 token to the left -- we have already seen this when training the RNN decoder for machine translation. This shifting is done with the line `tgt_out = tgt[1:, :]` by removing the `<SOS>` token from the start of all sequences. Since the input and output of the decoder have to have the same sequence lengths, the line `tgt_input = tgt[:-1, :]` removes the last item in all sequences, which is a `<PAD>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in tqdm(train_dataloader, total=len(list(train_dataloader))):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        # Remove last entry an all target sequences (typically PAD, can be EOS)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Remove <SOS> from all targets\n",
    "        tgt_out = tgt[1:, :]\n",
    "        \n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in tqdm(val_dataloader, total=len(list(val_dataloader))):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "    \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        \n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the ingredients to train our model. Let's do it!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "\n",
    "Let's now define the parameters of our model and instantiate the same. Below, we also define our loss function which is the cross-entropy loss and the optimizer used for training. Of course, we do not want to compute the loss w.r.t. to any padding tokens -- for example, see the last 4 `<PAD>` tokens of the decoder output in the figure above. While we could manually only consider the non-adding tokens of the decoder output, PyTorch allows us to specify which token index to ignore when defining the loss function. This means, we can tell the loss function to ignore the losses for each padding token using `ignore_index=PAD_IDX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "# Create model\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "# Initialize weights\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# Move model to device (ideally GPU, otherwise CPU)\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "In the code cell below, we perform the actual training by calling the method `train_epoch()` `NUM_EPOCHS` times. After each epoch we also call the `evaluate()` method to evaluate our current training model using the validation data. This gives us 2 losses after each iteration, the training loss and the evaluation loss, which are both displayed via a print statement together with the total time required to perform 1 iteration. This also means that you will have 2 progress bars for each iteration, again one for the training and one for the validation. If you want to train the model further after the code cell is completed, you can simply run the same code cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                        | 0/227 [00:00<?, ?it/s]/home/vdw/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 227/227 [00:30<00:00,  7.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 15.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.341, Val loss: 4.109, Epoch time (total) = 47.239s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 227/227 [00:23<00:00,  9.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 15.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 3.763, Val loss: 3.326, Epoch time (total) = 40.089s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 227/227 [00:23<00:00,  9.66it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer, criterion)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time (total) = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "\n",
    "Seeing particularly the validation loss going down is of course great, but at the end of the day, we actually want to use our model to translate sentences for here (here, from German to English). For this, the code cell below implements the method `translate()` which takes the train model and an input sentence as string as input and uses the trained model to translate the input sentence. The actual translation -- that is, the generation of the output sentence in the target language -- is done using the method `greedy_decode()`. As the name suggests, this method performs Greedy Decoding. This means that for each next word to be generated the method will always choose the one with the highest probability.\n",
    "\n",
    "**Side note:** Recall that a more sophisticated alternative to Greedy Decoding is Beam Search. Instead of picking inly the one word with the highest probability in each step, Beam Search picks multiple words (say, 10) in each step to maximize the probability of the whole sentence -- Greede Decoding does not guarantee this! Depending on the exact implementation of parametrization of Beam Search, the same input sentence might be translated in (slightly) different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# Actual method to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens+5, start_symbol=SOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<SOS>\", \"\").replace(\"<EOS>\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's translate a couple of sentences to see how well our model performs. But you need to keep in mind the nature of the Multi30k dataset. This dataset consists of typically short image captions/descriptions, i.e., typically short and simple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Ein Koch in weißer Uniform bereitet Essen in einer Restaurantküche zu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Zwei junge Mädchen spielen Fußball auf einem Feld. .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Frau mit Hut und Sonnenbrille steht am Strand .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Zwei Freunde lachen und genießen ein Eis auf einer wunderschönen Wiese .\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "After completing the notebook, training a Transformer for machine translation might seem rather easy to do. After all, it does not take much training to see good translation results. However, this is a bit misleading insofar we used only an arguably small dataset with generally simple sentences. In practice, training Transformers for machine translation poses several challenges. Here are some of the key difficulties:\n",
    "\n",
    "* **Data scarcity:** Training effective machine translation models requires large amounts of high-quality bilingual data. Acquiring such data can be expensive and time-consuming, particularly for low-resource language pairs. Limited data can lead to overfitting and poor generalization.\n",
    "\n",
    "* **Tokenization and subword units:** Transformers operate on sequences of fixed-length tokens. Deciding how to tokenize and represent words and phrases is crucial. Languages with complex morphology, agglutination, or lack of clear word boundaries pose challenges in determining appropriate subword units. Deciding on an effective tokenization strategy is essential for achieving accurate translations.\n",
    "\n",
    "* **Vocabulary mismatch:** Machine translation models are sensitive to the vocabulary mismatch between the source and target languages. Words and phrases that exist in one language may not have direct equivalents in the other. Handling out-of-vocabulary (OOV) words or rare words that may appear during inference is a challenge. Subword-based approaches partially alleviate this issue, but it is not entirely eliminated.\n",
    "\n",
    "* **Rare and ambiguous translations:** Some translations are rare, context-dependent, or highly ambiguous. For example, the same source phrase can have multiple valid translations depending on the context. Capturing the correct meaning and selecting appropriate translations require the model to understand the source sentence's semantics and the target language's nuances.\n",
    "\n",
    "* **Long-range dependencies:** Translations often require understanding long-range dependencies between words or phrases. Traditional recurrent neural networks (RNNs) struggle to capture these dependencies efficiently. Transformers address this issue by employing self-attention mechanisms that allow capturing dependencies across long distances, but training them effectively can still be challenging.\n",
    "\n",
    "* **Biases and cultural differences:** Machine translation models can unintentionally amplify biases present in the training data, leading to biased translations. Addressing biases and ensuring fair and culturally sensitive translations is an ongoing challenge.\n",
    "\n",
    "* **Computational resources:** Training transformers for machine translation is computationally intensive. Transformers have a large number of parameters and require substantial computational resources, including powerful GPUs or specialized hardware, to train effectively. Training on massive-scale models further amplifies the resource requirements.\n",
    "\n",
    "Overcoming these challenges requires a combination of large-scale, high-quality training data, careful preprocessing and tokenization, effective modeling techniques, architecture modifications, regularization methods, and continuous research and development efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Machine translation using the Transformer architecture has revolutionized the field by achieving state-of-the-art results. The Transformer is a neural network architecture that eliminates the need for recurrent or convolutional layers, making it highly parallelizable and efficient. It relies on a self-attention mechanism to capture dependencies between words in a sentence, allowing it to handle long-range relationships effectively. In a nutshell, the Transformer architecture for machine translation consists of an encoder and a decoder. The encoder processes the input sentence in the source language, generating a rich representation of the sentence. The decoder then uses this representation to generate the translation in the target language. Both the encoder and decoder are composed of multiple layers of self-attention and feed-forward neural networks.\n",
    "\n",
    "The self-attention mechanism in the Transformer enables the model to attend to different positions in the input sentence while generating the translation. By attending to relevant words and their dependencies, the Transformer captures contextual information effectively. Additionally, the use of residual connections and layer normalization helps alleviate vanishing gradient problems and aids in smoother training. During training, the Transformer architecture is optimized using a variant of the attention mechanism called \"scaled dot-product attention.\" This attention mechanism allows the model to assign appropriate weights to different words in the input sequence, enabling accurate translation. The model is trained to minimize the discrepancy between the predicted translation and the reference translation using techniques such as teacher forcing or reinforcement learning.\n",
    "\n",
    "Machine translation with the Transformer architecture has demonstrated remarkable performance on various language pairs, achieving human-level or even surpassing human-level translation quality. Its ability to capture long-range dependencies, parallel processing, and effective attention mechanism makes it a powerful tool in the field of machine translation, enabling accurate and fluent translations across different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
